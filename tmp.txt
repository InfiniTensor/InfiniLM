[CPP getPatchEmbedWeight] vision_patch_embed_weight address: =0x7fa938218998
[CPP getPatchEmbedWeight] Detected dtype: INFINI_DTYPE_F16
[CPP getPatchEmbedWeight] Calculated shape: [1024, 3, 14, 14]
Tensor: Tensor: shape[ 1024 3 14 14 ] strides[ 588 196 14 1 ] dtype=12 device=8 device_id=0
Loading model weights to host...
Model config: {'architectures': ['LlavaForConditionalGeneration'], 'ignore_index': -100, 'image_token_index': 32000, 'model_type': 'llava', 'pad_token_id': 32001, 'projector_hidden_act': 'gelu', 'text_config': {'_name_or_path': 'lmsys/vicuna-7b-v1.5', 'architectures': ['LlamaForCausalLM'], 'max_position_embeddings': 4096, 'model_type': 'llama', 'rms_norm_eps': 1e-05, 'torch_dtype': 'float16', 'vocab_size': 32064}, 'tie_word_embeddings': False, 'torch_dtype': 'float16', 'transformers_version': '4.36.0.dev0', 'vision_config': {'hidden_size': 1024, 'image_size': 336, 'intermediate_size': 4096, 'model_type': 'clip_vision_model', 'num_attention_heads': 16, 'num_hidden_layers': 24, 'patch_size': 14, 'projection_dim': 768, 'vocab_size': 32000}, 'vision_feature_layer': -2, 'vision_feature_select_strategy': 'default', 'vocab_size': 32064}
Model eos_token_id: [2]
Loading LLaVA model...
state_dict keys: ['language_model.model.embed_tokens.weight', 'language_model.model.layers.0.input_layernorm.weight', 'language_model.model.layers.0.mlp.down_proj.weight', 'language_model.model.layers.0.mlp.gate_proj.weight', 'language_model.model.layers.0.mlp.up_proj.weight', 'language_model.model.layers.0.post_attention_layernorm.weight', 'language_model.model.layers.0.self_attn.k_proj.weight', 'language_model.model.layers.0.self_attn.o_proj.weight', 'language_model.model.layers.0.self_attn.q_proj.weight', 'language_model.model.layers.0.self_attn.v_proj.weight'] ...
[Python LlavaWeightsImpl] torch_dt_mat: torch.float16 
[Python LlavaWeightsImpl] vision_patch_embed_tensor shape: torch.Size([1024, 3, 14, 14]) 
[Python LlavaWeightsImpl] first 10 vision_patch_embed_weight: tensor([0.0246, 0.0103, 0.0072, 0.0008, 0.0315, 0.0370, 0.0265, 0.0102, 0.0082,
        0.0029], dtype=torch.float16) 
[Python LlavaWeightsImpl] vision_patch_embed_weight address: 0x7fa938218998
Time used: 0.042s
Creating model on 1 devices...
Time used: 0.109s
Input token IDs shape: torch.Size([1, 593])
KV Cache: None
=== LLaVA Vision Encoding ===
Vision encoding: input shape torch.Size([1, 3, 336, 336]) -> features size 589824
pixel_values first 10: 1.506982, 1.506982, 1.506982, 1.506982, 1.506982, 1.506982, 1.506982, 1.506982, 1.506982, 1.506982
[CPP getPatchEmbedWeight] Run debug.
[CPP createPositionEmbedding] Shape: [577, 1024]
[CPP createClassToken] Shape: [1, 1024]
inferBatchLlavaVison called: image_data=0x5645d8f5f770, output=0x5645dac92f00
Vision config: embed_dim=1024, num_patches=576, total_features=589824
DEBUG: Running Conv2d: input [1,3,336,336] -> output [1,576,1024]
DEBUG: vision_patch_embed_weight = 0x7faa800534f0
inferBatchLlavaVison: vision encoding completed
