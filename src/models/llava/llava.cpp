#include "llava_impl.hpp"
#include "llava_weight.hpp"

#include "../../tensor.hpp"
#include "../../utils.hpp"
#include "../inference_context.hpp"
#include "infinicore_infer/models/llava.h"

#include <random>
#include <thread>
#include <vector>
#include <fstream>
#include <iomanip>

// LLaVAè®¾å¤‡èµ„æºåˆ›å»ºå‡½æ•°ï¼Œæ¨¡ä»¿jiuge.cppçš„createDeviceResource
void createLlavaDeviceResource(LlavaDeviceResource *rsrc, const LlavaMeta *meta,
                             const LlavaWeights *weights,
                             infiniDevice_t device, int idev, int ndev, int dev_id,
                             infinicclComm_t comm) {
    RUN_INFINI(infinirtSetDevice(device, dev_id));
    infiniopHandle_t handle;
    infiniopCreateHandle(&handle);
    infinirtStream_t stream;
    infinirtStreamCreate(&stream);

    // åˆå§‹åŒ–memory_pool
    auto memory_pool = std::make_shared<MemoryPool>(128 * 1024 * 1024);

    // åˆå§‹åŒ–Language Modelæƒé‡ï¼ˆæš‚æ—¶ä¸ºç©ºï¼Œå¤ç”¨jiugeç»“æ„ï¼‰
    std::vector<std::shared_ptr<Tensor>> w_attn_norm, w_attn_qkv, b_attn_qkv, w_attn_q_norm, w_attn_k_norm, w_attn_out,
        w_ffn_norm, w_ffn_gate_up, w_ffn_down;

    // åˆå§‹åŒ–Vision Encoderæƒé‡
    auto vision_patch_embed_weight = getPatchEmbedWeight(meta, weights);
    auto vision_position_embedding = createPositionEmbedding(meta, weights); // ä»metaä¸­è·å–å½¢çŠ¶
    auto vision_class_token = getClassToken(meta, weights); // ä»metaä¸­è·å–å½¢çŠ¶
    auto vision_pre_layernorm_weight = getVisionPreLNWeight(meta, weights);
    auto vision_pre_layernorm_bias   = getVisionPreLNBias(meta, weights);

    auto vision_post_layernorm_weight = getVisionPostLNWeight(meta, weights);
    auto vision_post_layernorm_bias   = getVisionPostLNBias(meta, weights);

    std::vector<std::shared_ptr<Tensor>> vision_q_weights, vision_q_biases,
        vision_k_weights, vision_k_biases,
        vision_v_weights, vision_v_biases,
        vision_in_layer_pre_norm_weights, vision_in_layer_pre_norm_biases,
        vision_proj_weight, vision_proj_bias,
        vision_in_layer_post_norm_weight, vision_post_norm_bias,
        vision_mlp_fc1_weight, vision_mlp_fc1_bias,
        vision_mlp_fc2_weight, vision_mlp_fc2_bias;


    for (size_t layer = 0; layer < meta->vision_meta.vision_num_layers; layer++) {
        vision_q_weights.push_back(
            getVisionQWeight(meta, weights, layer));
        vision_q_biases.push_back(
            getVisionQBias(meta, weights, layer));
        vision_k_weights.push_back(
            getVisionKWeight(meta, weights, layer));
        vision_k_biases.push_back(
            getVisionKBias(meta, weights, layer));
        vision_v_weights.push_back(
            getVisionVWeight(meta, weights, layer));
        vision_v_biases.push_back(
            getVisionVBias(meta, weights, layer));
        // in-layer pre norm
        vision_in_layer_pre_norm_weights.push_back(
            getVisionInLayerPreNormWeight(meta, weights, layer));
        vision_in_layer_pre_norm_biases.push_back(
            getVisionInLayerPreNormBias(meta, weights, layer));

        // proj
        vision_proj_weight.push_back(
            getVisionProjWeight(meta, weights, layer));
        vision_proj_bias.push_back(
            getVisionProjBias(meta, weights, layer));

        // post norm
        vision_in_layer_post_norm_weight.push_back(
            getVisionInLayerPostNormWeight(meta, weights, layer));
        vision_post_norm_bias.push_back(
            getVisionInLayerPostNormBias(meta, weights, layer));

        // MLP fc1
        vision_mlp_fc1_weight.push_back(
            getVisionMLPFC1Weight(meta, weights, layer));
        vision_mlp_fc1_bias.push_back(
            getVisionMLPFC1Bias(meta, weights, layer));

        // MLP fc2
        vision_mlp_fc2_weight.push_back(
            getVisionMLPFC2Weight(meta, weights, layer));
        vision_mlp_fc2_bias.push_back(
            getVisionMLPFC2Bias(meta, weights, layer));

    }


    // auto vision_class_embedding = getClassToken(meta);

    // ä¸´æ—¶åˆ›å»ºlanguage modelæƒé‡ï¼ˆå°†æ¥åº”è¯¥ä»weightsä¸­åŠ è½½ï¼‰
    std::shared_ptr<Tensor> w_in_embd = nullptr;
    std::shared_ptr<Tensor> w_out_norm = nullptr;
    std::shared_ptr<Tensor> w_out_embd = nullptr;
    std::shared_ptr<Tensor> sin_table = nullptr;
    std::shared_ptr<Tensor> cos_table = nullptr;

    *rsrc = LlavaDeviceResource{
        device,
        dev_id,
        handle,
        w_in_embd, w_out_norm, w_out_embd, sin_table, cos_table,
        w_attn_norm, w_attn_qkv, b_attn_qkv, w_attn_q_norm, w_attn_k_norm, w_attn_out,
        w_ffn_norm, w_ffn_gate_up, w_ffn_down,
        vision_patch_embed_weight,
        vision_position_embedding,
        vision_class_token,
        vision_pre_layernorm_weight, vision_pre_layernorm_bias,
        vision_post_layernorm_weight, vision_post_layernorm_bias,
        vision_q_weights, vision_q_biases,
        vision_k_weights, vision_k_biases,
        vision_v_weights, vision_v_biases,
        vision_in_layer_pre_norm_weights, vision_in_layer_pre_norm_biases,
        vision_proj_weight, vision_proj_bias,
        vision_in_layer_post_norm_weight, vision_post_norm_bias,
        vision_mlp_fc1_weight, vision_mlp_fc1_bias,
        vision_mlp_fc2_weight, vision_mlp_fc2_bias,
        stream,
        comm,
        memory_pool,
    };
    RUN_INFINI(infinirtDeviceSynchronize());
}

void releaseDeviceResource(LlavaDeviceResource &res) {
    infinirtDeviceSynchronize();
    // Release individual Tensors
    res.w_in_embd.reset();
    res.w_out_norm.reset();
    res.w_out_embd.reset();
    res.sin_table.reset();
    res.cos_table.reset();
    for (auto &t : res.w_attn_norm) {
        t.reset();
    }
    res.w_attn_norm.clear();
    for (auto &t : res.w_attn_qkv) {
        t.reset();
    }
    res.w_attn_qkv.clear();
    for (auto &t : res.b_attn_qkv) {
        t.reset();
    }
    res.b_attn_qkv.clear();
    for (auto &t : res.w_attn_out) {
        t.reset();
    }
    res.w_attn_out.clear();
    for (auto &t : res.w_ffn_norm) {
        t.reset();
    }
    res.w_ffn_norm.clear();
    for (auto &t : res.w_ffn_gate_up) {
        t.reset();
    }
    res.w_ffn_gate_up.clear();
    for (auto &t : res.w_ffn_down) {
        t.reset();
    }
    res.w_ffn_down.clear();
    infiniopDestroyHandle(res.handle);
    res.handle = nullptr;
    infinirtStreamDestroy(res.stream);
    res.stream = nullptr;
    infinicclCommDestroy(res.comm);
    res.comm = nullptr;
}


// LLaVAè§†è§‰ç¼–ç è®¾å¤‡å±‚æ¨ç†å‡½æ•°ï¼ˆæ¨¡ä»¿inferDeviceBatchï¼‰
void inferDeviceBatchVision(const LlavaMeta &meta, LlavaDeviceResource &rsrc,
                           uint32_t idev, uint32_t ndev,
                           const void *image_data, uint32_t *output) {

// inputs["input_ids"].shape: torch.Size([1, 593])
// shape of weight: torch.Size([1024, 3, 14, 14])
// shape of input: torch.Size([1, 3, 336, 336])
// shape of output: torch.Size([1, 1024, 24, 24])
// Debug: print image_data pointer

    // === 1. å‡†å¤‡å‚æ•° ===
    auto vision_embed_dim = meta.vision_meta.vision_embed_dim; // 1024
    auto vision_nh   = meta.vision_meta.vision_num_heads; // 16
    auto image_size = meta.vision_meta.image_size; // 336
    auto patch_size = meta.vision_meta.patch_size; // 14
    auto dt_logits = meta.language_meta.dt_logits; // F16
    auto stream = rsrc.stream;
    // auto vision_num_layers = meta.vision_meta.vision_num_layers; // 24
    // è®¡ç®—patchesæ•°é‡
    auto patches_per_dim = image_size / patch_size; // 24
    auto total_patches = patches_per_dim * patches_per_dim; // 576
    auto vision_intermediate_size = meta.vision_meta.vision_intermediate_size; // 4096




    // å‡è®¾ä½ å·²ç»å¾—åˆ°äº† q_buf, k_buf, v_buf  shape = [1, seq_len, vision_embed_dim]
    // ç°åœ¨ reshape æˆå¤šå¤´æ ¼å¼
    auto vision_dh   = vision_embed_dim / vision_nh;
    auto vision_seq  = 1 + total_patches; // 577











    // === 2. å‡†å¤‡buffer ===
    // auto input_image_tensor_f32 = Tensor::buffer(INFINI_DTYPE_F32, {1, 3, image_size, image_size}, rsrc.memory_pool);
    auto input_image_tensor = Tensor::buffer(dt_logits, {1, 3, image_size, image_size}, rsrc.memory_pool);
    auto patch_embed_output = Tensor::buffer(dt_logits, {1, vision_embed_dim, patches_per_dim, patches_per_dim}, rsrc.memory_pool);
    // embeddings = torch.cat([class_embeds, patch_embeds], dim=1)
    auto embeddings = Tensor::buffer(dt_logits, {1, 1 + total_patches, vision_embed_dim}, rsrc.memory_pool);
    // [ 1 577 1024 ]
    auto pre_layernorm = Tensor::buffer(dt_logits, {1, 1 + total_patches, vision_embed_dim}, rsrc.memory_pool);
    auto vision_residual = Tensor::buffer(dt_logits, {1, 1 + total_patches, vision_embed_dim}, rsrc.memory_pool);
    auto in_layer_pre_norm = Tensor::buffer(dt_logits, {1, 1 + total_patches, vision_embed_dim}, rsrc.memory_pool);
    // [ 1 577 1024 ]
    auto q_buf = Tensor::buffer(dt_logits, {1, 1 + total_patches, vision_embed_dim}, rsrc.memory_pool);
    auto k_buf = Tensor::buffer(dt_logits, {1, 1 + total_patches, vision_embed_dim}, rsrc.memory_pool);
    auto v_buf = Tensor::buffer(dt_logits, {1, 1 + total_patches, vision_embed_dim}, rsrc.memory_pool);
    auto input_standardization = Tensor::buffer(dt_logits, {1, 1 + total_patches, vision_embed_dim}, rsrc.memory_pool);
    auto input_std_deviation   = Tensor::buffer(dt_logits, {1, 1 + total_patches}, rsrc.memory_pool);





    // å¤åˆ¶è¾“å…¥å›¾åƒæ•°æ®
    RUN_INFINI(infinirtMemcpyAsync(input_image_tensor->data(), image_data,
                                  image_size * image_size * 3 * sizeof(uint16_t),
                                  INFINIRT_MEMCPY_H2D, stream));

    // printf("DEBUG: input_image_tensor after memcpy:\n");
    // input_image_tensor->debug_first_n(10);

    // === 3. CLIPVisionEmbeddings Forward ===
    // Step 1: Patch Embedding (Conv2d)

    printf("DEBUG: Running Conv2d: input [1,3,%ld,%ld] -> output [1,%ld,%ld,%ld]\n",
           image_size, image_size, vision_embed_dim, patches_per_dim, patches_per_dim);

    // å‡†å¤‡å·ç§¯å‚æ•°
    std::vector<size_t> pads = {0, 0};  // æ— padding
    std::vector<size_t> strides = {static_cast<size_t>(patch_size), static_cast<size_t>(patch_size)};
    std::vector<size_t> dilations = {1, 1};

    // patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # Conv2d
    conv2d(patch_embed_output, input_image_tensor, rsrc.vision_patch_embed_weight,
           nullptr, pads, strides, dilations); // ï¼ˆ1ï¼Œ1024ï¼Œ24ï¼Œ24ï¼‰

    // flatten 2D patch -> [batch, embed_dim, total_patches]
    auto patch_embed_flat = patch_embed_output->view({1, vision_embed_dim, total_patches});

    // transpose -> [batch, total_patches, embed_dim]
    auto patch_embed_transposed = patch_embed_flat->permute({0, 2, 1});
    // åˆ›å»º class embedding buffer
    // class_embeds = self.class_embedding.expand(batch_size, 1, -1)
    // assume batch=1

    auto class_embed_tensor = Tensor::buffer(dt_logits, {1, 1, vision_embed_dim}, rsrc.memory_pool); 
    // Tensor: shape[ 1 1 1024 ]
    RUN_INFINI(infinirtMemcpyAsync(class_embed_tensor->data(),
                                rsrc.vision_class_token->data(),
                                sizeof(uint16_t) * vision_embed_dim,
                                INFINIRT_MEMCPY_D2D, stream));

    // 1) æŠŠ class token æ”¾åˆ° embeddings[:, 0:1, :]
    rearrange(embeddings->slice(1, 0, 1), class_embed_tensor); // æ³¨æ„ï¼šslice(dim=1, start=0, length=1)
    // 2) æŠŠæ‰€æœ‰ patch token æ”¾åˆ° embeddings[:, 1:1+T, :]
    rearrange(embeddings->slice(1, 1, total_patches), patch_embed_transposed); // æ³¨æ„ï¼šslice(dim=1, start=1, length=total_patches)

    // 3) åŠ  position embedding ï¼ˆpos tensor å¿…é¡»æ˜¯ [1, 1+T, C]ï¼‰
    add(embeddings, embeddings, rsrc.vision_position_embedding);
    // printf("DEBUG: embeddings after add position embedding:\n");
    // embeddings->debug_first_n(10);
    // embeddings->debug();

    // (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True) # æš‚æœªå®ç°
    printf("meta.vision_meta.vision_epsilon: %e\n", meta.vision_meta.vision_epsilon);
    layernorm(/*out_put*/ pre_layernorm,
                /*input_standardization*/ input_standardization,
                /*input_std_deviation*/ input_std_deviation,
                /*input*/ embeddings,
                /*weight*/ rsrc.vision_pre_layernorm_weight,
                /*bias*/ rsrc.vision_pre_layernorm_bias,
                meta.vision_meta.vision_epsilon); // 1e-5
    // printf("DEBUG: pre_layernorm after LayerNorm_1\n");
    // pre_layernorm->debug_first_n(10);



    // for (uint32_t layer = 0; layer < vision_num_layers; layer++) {
    for (uint32_t layer = 0; layer < 1; layer++) {

        // residual = hidden_states
        // vision_residual = pre_layernorm;
        RUN_INFINI(infinirtMemcpyAsync(vision_residual->data(),
                                    pre_layernorm->data(),
                                    sizeof(dt_logits) * (1 + total_patches) * vision_embed_dim,
                                    INFINIRT_MEMCPY_D2D, stream));
        // printf("DEBUG: pre_layernorm:\n");
        // pre_layernorm->debug_first_n(10);
        // printf("DEBUG: vision_residual:\n");
        // vision_residual->debug_first_n(10);

        // (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True))

        std::cout << "q_buf->info()" << q_buf->info() << std::endl;
        layernorm(/*out_put*/ in_layer_pre_norm,
                    /*input_standardization*/ input_standardization,
                    /*input_std_deviation*/ input_std_deviation,
                    /*input*/ pre_layernorm,
                    /*weight*/ rsrc.vision_in_layer_pre_norm_weights[layer],
                    /*bias*/ rsrc.vision_in_layer_pre_norm_biases[layer],
                    meta.vision_meta.vision_epsilon); // 1e-5
        printf("DEBUG: in_layer_pre_norm after LayerNorm_2\n");
        in_layer_pre_norm->debug_first_n(10); 
        // debug: ä¸è€ƒè™‘ä¸­é—´ä¸¤è¡Œï¼Œè¿™é‡Œæ˜¯å¯¹çš„äº†ã€‚(== hidden_states at encoder_layer start__3)

        // // æµ‹è¯•äºŒç»´çš„linearå’Œä¸‰ç»´çš„linearæ˜¯å¦ä¸€æ ·
        // std::cout << "q_buf->info()" << q_buf->info() << std::endl;
        // // shape[ 1 577 1024 ]
        // std::cout << "pre_layernorm->info()" << pre_layernorm->info() << std::endl;
        // // shape[ 1 577 1024 ]
        // std::cout << "rsrc.vision_q_weights[layer]->info()" << rsrc.vision_q_weights[layer]->info() << std::endl;
        // // shape[ 1024 1024 ]
        // // biasåº”è¯¥æ˜¯ shape[ 1024 ]ï¼Œæ­£ç¡®æ€§debugçš„æ—¶å€™å¯ä»¥å»linearé‡Œçœ‹çœ‹biasè¢«æ‹“å±•æˆä»€ä¹ˆå½¢çŠ¶äº†
        // // å½“å‰è¿™ä¹ˆä¹˜ï¼Œè·Ÿé™ç»´åçš„ç»“æœï¼Œè¿˜æ˜¯åªæœ‰ä¸¤è¡Œä¸ä¸€æ ·â€¦â€¦å¥½å¥‡æ€ªï¼Œä½†åº”è¯¥è¿˜æ˜¯æœ€å¼€å§‹é‚£ä¸ªå•¥å¯¼è‡´çš„ã€‚


        // // çº¿æ€§æŠ•å½±
        linear(q_buf, in_layer_pre_norm, rsrc.vision_q_weights[layer]->permute({1, 0}), 1.0, 0.0, nullptr, rsrc.vision_q_biases[layer]);
        // printf("DEBUG: q_buf after linear projection:\n");
        // // debug: ä¸è€ƒè™‘ä¸­é—´ä¸¤è¡Œï¼Œè¿™é‡Œæ˜¯å¯¹çš„äº†ã€‚(== queries (first 10 elements): )
        // q_buf->debug();
        linear(k_buf, in_layer_pre_norm, rsrc.vision_k_weights[layer]->permute({1, 0}), 1.0, 0.0, nullptr, rsrc.vision_k_biases[layer]);
        linear(v_buf, in_layer_pre_norm, rsrc.vision_v_weights[layer]->permute({1, 0}), 1.0, 0.0, nullptr, rsrc.vision_v_biases[layer]);




        // 1) rearrange Q/K/V â†’ [vision_nh, vision_seq, vision_dh]
        auto q_rearr = Tensor::buffer(dt_logits, {1, vision_nh, vision_seq, vision_dh}, rsrc.memory_pool);
        auto k_rearr = Tensor::buffer(dt_logits, {1, vision_nh, vision_seq, vision_dh}, rsrc.memory_pool);
        auto v_rearr = Tensor::buffer(dt_logits, {1, vision_nh, vision_seq, vision_dh}, rsrc.memory_pool);
        
        // std::cout << "q_rearr->info()" << q_rearr->info() << std::endl;
        // printf("DEBUG: Rearranging Q/K/V tensors\n");
        // auto test = q_buf->view({1, vision_seq, vision_nh, vision_dh});
        // std::cout << "test->info()" << test->info() << std::endl;
        // auto test_perm = test->permute({0,2,1,3});
        // std::cout << "test_perm->info()" << test_perm->info() << std::endl;
        // printf("DEBUG: Rearranging Q/K/V tensors\n");



        rearrange(q_rearr, q_buf->view({1, vision_seq, vision_nh, vision_dh})->permute({0,2,1,3}));
        rearrange(k_rearr, k_buf->view({1, vision_seq, vision_nh, vision_dh})->permute({0,2,1,3}));
        rearrange(v_rearr, v_buf->view({1, vision_seq, vision_nh, vision_dh})->permute({0,2,1,3}));

        // 2) å‡†å¤‡ QK = [vision_nh, vision_seq, vision_seq]
        auto qk_buf = Tensor::buffer(dt_logits, {vision_nh, vision_seq, vision_seq}, rsrc.memory_pool);

        // 3) Q * K^T + scaling
        auto k_T = k_rearr->permute({0,1,3,2});  // [vision_nh, vision_dh, vision_seq]
        linear(
            qk_buf,
            q_rearr->slice(0, 0, 1)->view({vision_nh, vision_seq, vision_dh}),
            k_T->slice(0, 0, 1)->view({vision_nh, vision_dh, vision_seq}),
            /*alpha=*/0.125,   // <-- scalingï¼Œä¸¥æ ¼å’Œ torch ä¸€è‡´
            /*beta=*/0.0,
            nullptr,
            nullptr
        );

        // 4) softmax (ä½ è¿˜æ²¡å®ç°ï¼Œç”¨ causalSoftmax ä¸´æ—¶ä»£æ›¿)
        auto qk_softmax = qk_buf->view({vision_nh, vision_seq, vision_seq});
        causalSoftmax(qk_softmax, qk_softmax);  // debug: FIXME: non-causal softmax required

        // 5) Attn * V
        auto attn_val_buf = Tensor::buffer(dt_logits, {vision_nh, vision_seq, vision_dh}, rsrc.memory_pool);
        // auto v_gemm = v_rearr->permute({0,1,3,2});   // [vision_nh, vision_dh, vision_seq]
        auto v_gemm = v_rearr->permute({0,1,2,3});   // debug

        std::cout << "attn_val_buf->info()" << attn_val_buf->info() << std::endl;
        std::cout << "qk_softmax->info()" << qk_softmax->info() << std::endl;
        std::cout << "v_gemm->slice(0, 0, 1)->view({vision_nh, vision_dh, vision_seq})->info()" << v_gemm->slice(0, 0, 1)->view({vision_nh, vision_dh, vision_seq})->info() << std::endl;

        linear(
            attn_val_buf, // debug: shape[ 16 577 64 ] strides[ 36928 64 1 ]
            qk_softmax, // debug: shape[ 16 577 577 ] strides[ 332929 577 1 ] 
            v_gemm->slice(0, 0, 1)->view({vision_nh, vision_seq, vision_dh}), // debug: æ³¨æ„è¿™é‡Œçš„ view, å¯èƒ½ä¸å¯¹ã€shape[ 16 64 577 ] strides[ 36928 577 1 ]ã€‘
            /*alpha=*/1.0,
            /*beta=*/0.0,
            nullptr,
            nullptr
        );

        // 6) åˆå¤´ â†’ o: [1, vision_seq, vision_embed_dim]
        auto o_tmp = Tensor::buffer(dt_logits, {1, vision_seq, vision_nh, vision_dh}, rsrc.memory_pool);
        rearrange(o_tmp, attn_val_buf->view({1, vision_nh, vision_seq, vision_dh})->permute({0,2,1,3}));
        std::cout << "o_tmp->info()" << o_tmp->info() << std::endl; // Tensor: shape[ 1 577 16 64 ]
        auto o = Tensor::buffer(dt_logits, {1, vision_seq, vision_embed_dim}, rsrc.memory_pool);
        rearrange(o, o_tmp->view({1, vision_seq, vision_embed_dim}));
        std::cout << "o->info()" << o->info() << std::endl;


        // === Attention out_proj ===
        // o -> attn_out
        auto attn_out = Tensor::buffer(dt_logits, {1, vision_seq, vision_embed_dim}, rsrc.memory_pool);
        linear(attn_out, o, rsrc.vision_proj_weight[layer]->permute({1, 0}), 1.0f, 0.0f, nullptr, rsrc.vision_proj_bias[layer]);

        // === Attention residual add ===   // å¤ç”¨ pre_layernorm ä½œä¸ºè¾“å‡º buffer
        // hidden_states = residual + hidden_states
        add(pre_layernorm, attn_out, vision_residual);
        std::cout << pre_layernorm->info() << std::endl;
        // æ­¤æ—¶ pre_layernorm = attention block çš„è¾“å‡º

        // hidden_states = self.layer_norm2(hidden_states)
        auto post_attn_norm = Tensor::buffer(dt_logits, {1, vision_seq, vision_embed_dim}, rsrc.memory_pool);
        layernorm(
            /*out*/ post_attn_norm,
            /*input_standardization*/ input_standardization,
            /*input_std_deviation*/ input_std_deviation,
            /*input*/ pre_layernorm,
            /*weight*/ rsrc.vision_in_layer_post_norm_weight[layer],
            /*bias*/  rsrc.vision_post_norm_bias[layer],
            meta.vision_meta.vision_epsilon
        );

        // mlp_out = self.mlp(hidden_states)
        auto mlp_fc1_out = Tensor::buffer(dt_logits, {1, vision_seq, vision_intermediate_size}, rsrc.memory_pool);
        linear(
            mlp_fc1_out,
            post_attn_norm,
            rsrc.vision_mlp_fc1_weight[layer]->permute({1, 0}),
            1.0f,
            0.0f,
            nullptr,
            rsrc.vision_mlp_fc1_bias[layer]
        );

        // TODO: gelu activation
 


        // if(layer == 0) {
        //     // printf("DEBUG: After first layer linear projections shapes:\n");
        //     // std::cout << flat_q_buf->info() << std::endl;
        //     // std::cout << flat_embeddings->info() << std::endl;
        //     // std::cout << rsrc.vision_q_weights[layer]->info() << std::endl;
        //     // std::cout << rsrc.vision_q_biases[layer]->info() << std::endl;
        //     // printf("DEBUG: vision_q_weights");
        //     // rsrc.vision_q_weights[layer]->debug_first_n(10);
        //     // printf("DEBUG: vision_q_biases");
        //     // rsrc.vision_q_biases[layer]->debug_first_n(10);
        //     // printf("DEBUG: vision_k_weights");
        //     // rsrc.vision_k_weights[layer]->debug_first_n(10);
        //     // printf("DEBUG: vision_k_biases");
        //     // rsrc.vision_k_biases[layer]->debug_first_n(10);
        //     // rsrc.vision_v_weights[layer]->debug_first_n(10);
        //     // printf("DEBUG: vision_v_biases");
        //     // rsrc.vision_v_biases[layer]->debug_first_n(10);

        //     printf("DEBUG: After first layer linear projections:\n");
        //     // // q_buf->debug_first_n(10);
        //     // // k_buf->debug_first_n(10);
        //     // // v_buf->debug_first_n(10);
        //     q_buf->debug();
        //     // printf("\n\n\n\n\n");
        //     // k_buf->debug();
        //     // printf("\n\n\n\n\n");
        //     // v_buf->debug();
        // }
    }
    auto fake_output = Tensor::buffer(dt_logits, {1, vision_seq, vision_embed_dim}, rsrc.memory_pool);

}




// LLaVAè®¾å¤‡å·¥ä½œçº¿ç¨‹å‡½æ•°ï¼Œä¸¥æ ¼æŒ‰ç…§jiuge.cppçš„launchDeviceç»“æ„
void launchLlavaDevice(const LlavaMeta &meta, const LlavaWeights *weights,
                     LlavaDeviceResource *rsrc, LlavaInferState &state,
                     LlavaRequest &req,
                     infiniDevice_t device, int idev, int ndev, int dev_id,
                     infinicclComm_t comm) {
    // Create Device Resource
    // åˆå§‹åŒ–è®¾å¤‡èµ„æº
    createLlavaDeviceResource(rsrc, &meta, weights, device, idev, ndev, dev_id, comm);

    CacheManager cache_manager(100);
    InferenceContext ctx(rsrc->handle, rsrc->memory_pool, &cache_manager, rsrc->stream);
    setInferenceContext(&ctx);

    // é€šçŸ¥ä¸»çº¿ç¨‹ï¼šè¿™ä¸ªè®¾å¤‡å·²ç»åŠ è½½å®Œæˆ
    // TODO: æ²¡æœ‰æ£€æŸ¥ç°åœ¨æ ‡å¿—ä½æ˜¯å¦é è°±
    {
        std::unique_lock<std::mutex> lock(state.mtx);
        state.loaded = true;
        lock.unlock();
        state.cv_stage.notify_one();
    }

    // Infer Loop
    // è¿›å…¥æ¨ç†å¾ªç¯ï¼ˆè¿™ä¸ªçº¿ç¨‹ä¼šä¸€ç›´è¿è¡Œï¼‰
    while (true) {
        std::unique_lock<std::mutex> lock(state.mtx);
        // å…³é”®ç‚¹ï¼šçº¿ç¨‹åœ¨è¿™é‡Œåœä¸‹æ¥ç­‰å¾…ï¼
        state.cv_stage.wait(lock, [&] { return state.proceed || state.exit_flag; });
        // quit if exit_flag is set
        if (state.exit_flag) {
            break;  // é€€å‡ºçº¿ç¨‹
        }

        // TODO: æ‰§è¡Œæ¨ç†
        // // å ä½ç¬¦ï¼šç®€å•è¿”å›ä¸€ä¸ªtoken
        // if (req.output && req.batch_size > 0) {
        //     req.output[0] = 1;
        // }

        inferDeviceBatchVision(meta, *rsrc, idev, ndev, 
                                req.image_data, req.output);

        // // === LLaVAå››é˜¶æ®µæ¨ç†æµç¨‹ ===
        // // é˜¶æ®µ1: Vision Encoder (å¦‚æœæœ‰å›¾åƒ)
        // if (req.image_data != nullptr) {
        //     state.current_stage = 1;
        //     state.stage_completed = false;
        //     lock.unlock();
        //     state.cv_stage.notify_one(); // é€šçŸ¥ä¸»çº¿ç¨‹è¿›å…¥é˜¶æ®µ1

        //     // TODO: å®ç°vision encoding
        //     // encodeVisionFeatures(meta, *rsrc, req.image_data, state.vision_features);

        //     lock.lock();
        //     state.stage_completed = true;
        //     state.current_stage = 2;
        // }

        // // é˜¶æ®µ2: MultiModal Projector (å¦‚æœæœ‰å›¾åƒç‰¹å¾)
        // if (state.vision_features != nullptr) {
        //     lock.unlock();
        //     state.cv_stage.notify_one(); // é€šçŸ¥ä¸»çº¿ç¨‹è¿›å…¥é˜¶æ®µ2

        //     // TODO: å®ç°multimodal projection
        //     // projectMultiModalFeatures(meta, *rsrc, state.vision_features, state.projected_features);

        //     lock.lock();
        //     state.stage_completed = true;
        //     state.current_stage = 3;
        // }

        // // é˜¶æ®µ3: Language Model Prefill (åŒ…å«KV-Cache)
        // state.current_stage = 3;
        // state.stage_completed = false;
        // lock.unlock();
        // state.cv_stage.notify_one(); // é€šçŸ¥ä¸»çº¿ç¨‹è¿›å…¥é˜¶æ®µ3

        // // TODO: å®ç°language model prefill
        // // è¿™é‡Œè°ƒç”¨Jiugeçš„æ¨ç†é€»è¾‘æ¥å¤„ç†text tokens + projected vision features
        // // inferDeviceBatchLanguage(meta, *rsrc, idev, ndev, req.input_tokens, req.ntok,
        // //                          req.req_lens, req.nreq, req.req_pos, req.kv_caches,
        // //                          req.temperature, req.topk, req.topp, req.output, nullptr);

        // lock.lock();
        // state.stage_completed = true;
        // state.current_stage = 4;

        // // é˜¶æ®µ4: KV-Cache Compression (å¯é€‰)
        // if (req.kv_caches != nullptr && state.stage_completed) {
        //     lock.unlock();
        //     state.cv_stage.notify_one(); // é€šçŸ¥ä¸»çº¿ç¨‹è¿›å…¥é˜¶æ®µ4

        //     // TODO: å®ç°KV-Cacheå‹ç¼© (Future: é›†æˆFastcache)
        //     // compressKVCaches(meta, *rsrc, req.kv_caches);

        //     lock.lock();
        //     state.stage_completed = true;
        // }

        // // ç®€å•å ä½ç¬¦ï¼šè¿”å›ä¸€ä¸ªtoken (ä¸´æ—¶)
        // if (req.output && req.batch_size > 0) {
        //     req.output[0] = 1; // æš‚æ—¶è¿”å›å›ºå®štoken
        // }



        state.proceed = false;  // é‡ç½®ä¿¡å·
        lock.unlock();
        // é€šçŸ¥ä¸»çº¿ç¨‹ï¼šè¿™ä¸ªè®¾å¤‡å®Œæˆäº†æ¨ç†
        state.cv_stage.notify_one();
    }
    // Clean-Up
    releaseDeviceResource(*rsrc);
    setInferenceContext(nullptr); // Clear the context when done
}



// // LLaVAå››é˜¶æ®µç»Ÿä¸€æ¨ç†å®ç°
// void LlavaModel::inferBatchLlava(const uint32_t* input_tokens, const void* image_data,
//                                 void** kv_caches, uint32_t batch_size,
//                                 uint32_t* output) {
//     // 1. è®¾ç½®æ¨ç†è¯·æ±‚å‚æ•°
//     req.input_tokens = input_tokens;
//     req.image_data = image_data;
//     req.kv_caches = kv_caches;
//     req.batch_size = batch_size;
//     req.ntok = batch_size; // ç®€åŒ–ï¼šå‡è®¾æ¯ä¸ªè¯·æ±‚åªæœ‰ä¸€ä¸ªtoken
//     req.nreq = 1; // ç®€åŒ–ï¼šå‡è®¾åªæœ‰ä¸€ä¸ªè¯·æ±‚
//     req.output = output;

//     // 2. å¯åŠ¨æ‰€æœ‰è®¾å¤‡çº¿ç¨‹
//     auto ndev = dev_resources.size();
//     for (size_t i = 0; i < ndev; i++) {
//         std::unique_lock<std::mutex> lock(states[i].mtx);
//         states[i].proceed = true;
//         lock.unlock();
//         states[i].cv_stage.notify_one(); // å‘å‡ºæ¨ç†å¼€å§‹ä¿¡å·
//     }

//     // 3. ç­‰å¾…æ‰€æœ‰è®¾å¤‡å®Œæˆ
//     for (size_t i = 0; i < ndev; i++) {
//         std::unique_lock<std::mutex> lock(states[i].mtx);
//         states[i].cv_stage.wait(lock, [&] { return !(states[i].proceed); });
//         lock.unlock();
//     }

//     // 4. æ¸…ç†è¯·æ±‚å‚æ•°
//     req.input_tokens = nullptr;
//     req.image_data = nullptr;
//     req.kv_caches = nullptr;
//     req.output = nullptr;
// }

// æ¨¡ä»¿jiuge.cppçš„LlavaModel constructor
LlavaModel::LlavaModel(const LlavaMeta *_meta, const LlavaWeights *weights,
                      infiniDevice_t device_, std::vector<int> device_ids) : meta(*_meta) {
    int ndev = int(device_ids.size());
    device = device_;
    dev_ids = device_ids;
    dev_resources = std::vector<LlavaDeviceResource>(ndev);  // æ¯ä¸ªè®¾å¤‡çš„èµ„æº
    states = std::vector<LlavaInferState>(ndev);              // æ¯ä¸ªè®¾å¤‡çš„çŠ¶æ€
    threads.resize(ndev);                                   // æ¯ä¸ªè®¾å¤‡çš„çº¿ç¨‹

    RUN_INFINI(infinirtInit());

    auto comms = std::vector<infinicclComm_t>(ndev, nullptr);
    if (ndev > 1) {
        RUN_INFINI(infinicclCommInitAll(device, comms.data(), ndev, dev_ids.data()));
    }

    // ğŸ§µğŸ§µğŸ§µ è¿™é‡Œåˆ›å»ºçº¿ç¨‹ï¼
    for (int i = 0; i < ndev; i++) {
        threads[i] = std::thread(
            launchLlavaDevice, 
            std::cref(meta), 
            weights, 
            &dev_resources[i], 
            std::ref(states[i]), 
            std::ref(req), 
            device, 
            i, 
            ndev, 
            dev_ids[i], 
            comms[i]);

        // â³ çº¿ç¨‹ç«‹å³å¯åŠ¨ï¼Œè¿›å…¥launchLlavaDeviceå‡½æ•°
        // ğŸ˜´ åœ¨cv_stage.wait()å¤„å¼€å§‹ä¼‘çœ ç­‰å¾…
    }

    // ç­‰å¾…æ‰€æœ‰è®¾å¤‡çº¿ç¨‹åŠ è½½å®Œæˆ - ä½¿ç”¨cv_loadä¸jiuge.cppä¿æŒä¸€è‡´
    for (int i = 0; i < ndev; i++) {
        std::unique_lock<std::mutex> lock(states[i].mtx);
        states[i].cv_stage.wait(lock, [&] { return states[i].loaded; });
        lock.unlock();
    }
}


// // æœ€ç®€å•çš„ç»Ÿä¸€æ¨ç†æ¥å£
// void LlavaModel::inferBatchLlava(const uint32_t* input_tokens, const void* image_data,
//                                void** kv_caches, const char* mode, uint32_t batch_size,
//                                uint32_t* output) {
//     // æš‚æ—¶åªæ˜¯å ä½ç¬¦å®ç°
//     if (output && batch_size > 0) {
//         output[0] = 1;  // è¿”å›ä¸€ä¸ªç®€å•çš„token
//     }
// }

// // å„é˜¶æ®µæ‰§è¡Œå‡½æ•°çš„å ä½ç¬¦å®ç°
// void LlavaModel::executeVisionStage() {
//     // å ä½ç¬¦
// }

// void LlavaModel::executePrefillStage() {
//     // å ä½ç¬¦
// }

// void LlavaModel::executeCompressStage() {
//     // å ä½ç¬¦
// }

// void LlavaModel::executeDecodeStage() {
//     // å ä½ç¬¦
// }

// void LlavaModel::workerLoop() {
//     // å ä½ç¬¦
// }




// API implementations - æ¨¡ä»¿jiuge.cppçš„createJiugeModel
__C struct LlavaModel *createLlavaModel(const LlavaMeta *meta,
                                        const LlavaWeights *weights,
                                        infiniDevice_t device,
                                        int ndev,
                                        const int *dev_ids) {
    std::vector<int> device_ids_vec(ndev);
    std::copy(dev_ids, dev_ids + ndev, device_ids_vec.begin());
    LlavaModel *model = new LlavaModel(meta, weights, device, device_ids_vec);
    return model;
}

__C void destroyLlavaModel(struct LlavaModel *model) {
    if (!model) {
        return;
    }

    auto ndev = model->dev_resources.size();

    // é€šçŸ¥æ‰€æœ‰è®¾å¤‡çº¿ç¨‹é€€å‡º
    for (size_t idev = 0; idev < ndev; idev++) {
        std::unique_lock<std::mutex> lock(model->states[idev].mtx);
        model->states[idev].exit_flag = true;
        lock.unlock();
        model->states[idev].cv_stage.notify_one();
    }

    // ç­‰å¾…æ‰€æœ‰çº¿ç¨‹ç»“æŸ
    for (size_t idev = 0; idev < ndev; idev++) {
        model->threads[idev].join();
    }

    delete model;
}

// C API: æ‰¹é‡è§†è§‰ç¼–ç ï¼ˆç”¨äºPythonæ¥å£ï¼‰
__C void inferBatchLlavaVison(struct LlavaModel *model,
                           const void *image_data,
                           void *output) {
    if (!model || !image_data || !output) {
        return;
    }

    // 1. è®¾ç½®æ¨ç†å‚æ•°ï¼ˆæ¨¡ä»¿inferBatchJiugeï¼‰
    // TODO: æ„Ÿè§‰è¿™é‡Œçš„reqç»“æ„å¯èƒ½è¦é€æ¸æ”¹çš„åƒ struct InferRequest 
    model->req.input_tokens = nullptr;  // vision encodingä¸éœ€è¦input_tokens
    model->req.image_data = image_data;
    model->req.kv_caches = nullptr;     // vision encodingä¸éœ€è¦kv_caches
    model->req.batch_size = 1;          // ç®€åŒ–ï¼šå‡è®¾batch_sizeä¸º1
    model->req.ntok = 0;               // vision encodingä¸éœ€è¦tokens
    model->req.nreq = 1;               // ç®€åŒ–ï¼šå‡è®¾ä¸€ä¸ªè¯·æ±‚
    model->req.output = (uint32_t*)output;  // å°†outputè½¬æ¢ä¸ºuint32_tæŒ‡é’ˆ

    //////////////////////////////////////////////
    auto vision_embed_dim = model->meta.vision_meta.vision_embed_dim;
    auto num_patches = model->meta.vision_meta.num_patches;
    auto total_features = vision_embed_dim * num_patches;
    printf("inferBatchLlavaVison called: image_data=%p, output=%p\n", image_data, output);
    printf("Vision config: embed_dim=%zu, num_patches=%zu, total_features=%zu\n",
           vision_embed_dim, num_patches, total_features);
    //////////////////////////////////////////////


    // 2. é€šçŸ¥æ‰€æœ‰è®¾å¤‡çº¿ç¨‹å¼€å§‹å·¥ä½œï¼ˆæ¨¡ä»¿inferBatchJiugeï¼‰
    // TODO: æ³¨æ„ï¼Œå’Œjiugeä¸ä¸€æ ·çš„åœ°æ–¹åœ¨äºï¼Œæˆ‘ä»¬è¿™é‡Œç°åœ¨åªæœ‰ä¸€ä¸ªä¿¡å·é‡
    for (size_t idev = 0; idev < model->dev_ids.size(); idev++) {
        std::unique_lock<std::mutex> lock(model->states[idev].mtx);
        model->states[idev].proceed = true;  // è®¾ç½®ä¿¡å·
        lock.unlock();
        model->states[idev].cv_stage.notify_one();  // å”¤é†’çº¿ç¨‹ï¼ˆLLaVAä½¿ç”¨cv_stageï¼‰
    }

    // 3. ç­‰å¾…æ‰€æœ‰è®¾å¤‡çº¿ç¨‹å®Œæˆå·¥ä½œï¼ˆæ¨¡ä»¿inferBatchJiugeï¼‰
    for (size_t i = model->dev_ids.size(); i > 0; i--) {
        auto idev = i - 1;
        std::unique_lock<std::mutex> lock(model->states[idev].mtx);
        model->states[idev].cv_stage.wait(lock, [&] { return !(model->states[idev].proceed); });
        lock.unlock();
    }

    printf("inferBatchLlavaVison: vision encoding completed\n");
}

// æš‚æ—¶æ³¨é‡Šæ‰å…¶ä»–å¤æ‚çš„APIå‡½æ•°ï¼Œåªä¿ç•™æœ€åŸºæœ¬çš„