#!/usr/bin/env python3
"""
Fixed Qwen3 Implementation - Using New qw C++ API
This version properly implements Qwen3 support using the dedicated qwen3 C++ API
with proper Q/K normalization and parameter mapping.

Key Improvements:
1. Uses dedicated Qwen3 API instead of fallback jiuge API
2. Handles Q/K normalization weights properly
3. Implements separate QKV projections
4. One-to-one parameter mapping following jiuge.py patterns
"""

from typing import List, Optional
import os
import sys
import time
import json
import torch
import transformers
from pathlib import Path
from ctypes import POINTER, c_float, c_int, c_uint, c_void_p, byref
import safetensors
import ctypes

# Set default device
torch.set_default_device("cpu")

# Import the proper Qwen3 API
try:
    from libinfinicore_infer import (
        Qwen3MetaCStruct,
        Qwen3WeightsCStruct,
        create_qwen3_model,
        destroy_qwen3_model,
        create_qwen3_kv_cache,
        drop_qwen3_kv_cache,
        infer_qwen3_batch,
        DataType,
        DeviceType,
        KVCacheCStruct,
    )
    QWEN3_API_AVAILABLE = True
    print("âœ“ Qwen3 C++ API available")
except ImportError as e:
    print(f"âš  Qwen3 C++ API not available: {e}")
    print("  This version requires the qw implementation")
    sys.exit(1)

from infer_task import Qwen3InferTask, Qwen3KVCache


class Qwen3WeightsNaming:
    """Qwen3-specific weight naming with Q/K normalization and separate QKV support"""
    
    def input_embd(self):
        return "model.embed_tokens.weight"

    def output_norm(self):
        return "model.norm.weight"

    def output_embd(self):
        return "lm_head.weight"

    def attn_norm(self, i):
        return f"model.layers.{i}.input_layernorm.weight"

    def attn_q(self, i):
        return f"model.layers.{i}.self_attn.q_proj.weight"

    def attn_k(self, i):
        return f"model.layers.{i}.self_attn.k_proj.weight"

    def attn_v(self, i):
        return f"model.layers.{i}.self_attn.v_proj.weight"

    def attn_o(self, i):
        return f"model.layers.{i}.self_attn.o_proj.weight"

    def ffn_norm(self, i):
        return f"model.layers.{i}.post_attention_layernorm.weight"

    def gate(self, i):
        return f"model.layers.{i}.mlp.gate_proj.weight"

    def up(self, i):
        return f"model.layers.{i}.mlp.up_proj.weight"

    def down(self, i):
        return f"model.layers.{i}.mlp.down_proj.weight"

    # Qwen3-specific Q/K normalization weights
    def q_norm(self, i):
        return f"model.layers.{i}.self_attn.q_norm.weight"

    def k_norm(self, i):
        return f"model.layers.{i}.self_attn.k_norm.weight"

    @staticmethod
    def match(state_dict):
        """Check if state_dict matches Qwen3 naming pattern"""
        has_basic = (
            "model.norm.weight" in state_dict
            and "model.layers.0.self_attn.q_proj.weight" in state_dict
        )
        # Qwen3 often has q_norm and k_norm weights
        has_qk_norm = (
            "model.layers.0.self_attn.q_norm.weight" in state_dict
            and "model.layers.0.self_attn.k_norm.weight" in state_dict
        )
        return has_basic and has_qk_norm

class Qwen3MetaFromConfig(Qwen3MetaCStruct):
    """Qwen3 metadata structure from model config"""
    
    def __init__(self, config, dtype=torch.float16, max_tokens=None):
        super().__init__()  # å…ˆè°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°
        
        if dtype == torch.float16:
            dt_value = DataType.INFINI_DTYPE_F16
        elif dtype == torch.float32:
            dt_value = DataType.INFINI_DTYPE_F32
        elif dtype == torch.bfloat16:
            dt_value = DataType.INFINI_DTYPE_BF16
        else:
            dt_value = DataType.INFINI_DTYPE_F16

        # è®¾ç½®å­—æ®µå€¼
        self.dt_logits = dt_value
        self.nlayer = config["num_hidden_layers"]
        self.d = config["hidden_size"]
        self.nh = config["num_attention_heads"]
        self.nkvh = (
            config["num_key_value_heads"]
            if "num_key_value_heads" in config
            else config["num_attention_heads"]
        )
        self.dh = config["hidden_size"] // config["num_attention_heads"]
        self.di = config["intermediate_size"]
        self.dctx = (
            config["max_position_embeddings"] if max_tokens is None else max_tokens
        )
        self.dvoc = config["vocab_size"]
        self.epsilon = config.get("rms_norm_eps", 1e-6)
        self.theta = config.get("rope_theta", 10000.0)
        self.bos_token = config.get("bos_token_id", 1)
        self.end_token = config.get("eos_token_id", 2)
        self.attn_dropout = config.get("attention_dropout", 0.0)
        self.tie_embd = config.get("tie_word_embeddings", True)
        
        self.torch_dtype_logits = dtype

class Qwen3WeightsImpl:
    """é‡æ–°è®¾è®¡çš„Qwen3æƒé‡å®ç°ç±»ï¼Œä½¿ç”¨ç»„åˆæ¨¡å¼è€Œéç»§æ‰¿"""
    
    def __init__(
        self,
        meta,
        naming,
        state_dict,
        torch_dt_mat=torch.float16,
        torch_dt_norm=torch.float32,
        ndev=1,
        transpose_weight=True,
    ):
        # æå–å…³é”®å‚æ•°
        nlayer = meta.nlayer
        nh = meta.nh
        nkvh = meta.nkvh
        dh = meta.dh
        d = meta.d
        di = meta.di
        
        # éªŒè¯è®¾å¤‡åˆ†å¸ƒçº¦æŸ
        assert nh % nkvh == 0, f"æ³¨æ„åŠ›å¤´æ•° {nh} å¿…é¡»æ˜¯KVå¤´æ•° {nkvh} çš„å€æ•°"
        assert nh % ndev == 0, f"æ³¨æ„åŠ›å¤´æ•° {nh} å¿…é¡»å¯è¢«è®¾å¤‡æ•° {ndev} æ•´é™¤"
        assert nkvh % ndev == 0, f"KVå¤´æ•° {nkvh} å¿…é¡»å¯è¢«è®¾å¤‡æ•° {ndev} æ•´é™¤"
        assert di % ndev == 0, f"ä¸­é—´ç»´åº¦ {di} å¿…é¡»å¯è¢«è®¾å¤‡æ•° {ndev} æ•´é™¤"
        
        torch_dt_logits = meta.torch_dtype_logits
        
        # åˆ›å»ºCç»“æ„ä½“ï¼Œè€Œéç»§æ‰¿å®ƒ
        self.c_struct = Qwen3WeightsCStruct()
        
        # ä¿å­˜æ‰€æœ‰å¼ é‡å¼•ç”¨ï¼Œé˜²æ­¢è¢«åƒåœ¾å›æ”¶
        self._tensor_refs = []
        
        # è®¾ç½®åŸºæœ¬å­—æ®µ
        self.c_struct.nlayer = nlayer
        self.c_struct.transpose_linear_weights = 1 if transpose_weight else 0
        
       # è®¾ç½®æ•°æ®ç±»å‹ - ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨æšä¸¾å€¼ï¼Œä¸è¦åŒ…è£…
        if torch_dt_mat == torch.float16:
            self.c_struct.dt_mat = DataType.INFINI_DTYPE_F16
        elif torch_dt_mat == torch.float32:
            self.c_struct.dt_mat = DataType.INFINI_DTYPE_F32
        elif torch_dt_mat == torch.bfloat16:
            self.c_struct.dt_mat = DataType.INFINI_DTYPE_BF16
        else:
            raise ValueError("ä¸æ”¯æŒçš„æŠ•å½±æƒé‡æ•°æ®ç±»å‹")
            
        if torch_dt_norm == torch.float16:
            self.c_struct.dt_norm = DataType.INFINI_DTYPE_F16
        elif torch_dt_norm == torch.float32:
            self.c_struct.dt_norm = DataType.INFINI_DTYPE_F32
        elif torch_dt_norm == torch.bfloat16:
            self.c_struct.dt_norm = DataType.INFINI_DTYPE_BF16
        else:
            raise ValueError("ä¸æ”¯æŒçš„å½’ä¸€åŒ–æƒé‡æ•°æ®ç±»å‹")

        # ç¡®å®šè¾“å…¥/è¾“å‡ºåµŒå…¥åç§°
        input_embd_naming = (
            naming.input_embd()
            if naming.input_embd() in state_dict
            else naming.output_embd()
        )
        output_embd_naming = (
            naming.output_embd()
            if naming.output_embd() in state_dict
            else naming.input_embd()
        )
        
        # ---- åŸºç¡€æƒé‡åŠ è½½ ----
        # è¾“å…¥åµŒå…¥
        input_embd_tensor = state_dict[input_embd_naming].to(torch_dt_logits)
        self._tensor_refs.append(input_embd_tensor)
        self.c_struct.input_embd = input_embd_tensor.data_ptr()
        
        # è¾“å‡ºå½’ä¸€åŒ–
        output_norm_tensor = state_dict[naming.output_norm()].to(torch_dt_norm)
        self._tensor_refs.append(output_norm_tensor)
        self.c_struct.output_norm = output_norm_tensor.data_ptr()
        
        # è¾“å‡ºåµŒå…¥
        output_embd_tensor = state_dict[output_embd_naming].to(torch_dt_mat)
        if not transpose_weight:
            output_embd_tensor = output_embd_tensor.transpose(0, 1).contiguous()
        self._tensor_refs.append(output_embd_tensor)
        self.c_struct.output_embd = output_embd_tensor.data_ptr()

        # ---- æ³¨æ„åŠ›å±‚å½’ä¸€åŒ–æƒé‡ ----
        attn_norm_tensors = [
            state_dict[naming.attn_norm(i)].to(torch_dt_norm) for i in range(nlayer)
        ]
        self._tensor_refs.extend(attn_norm_tensors)
        attn_norm_ptrs = [tensor.data_ptr() for tensor in attn_norm_tensors]
        self.c_struct.attn_norm = (c_void_p * nlayer)(*attn_norm_ptrs)

        # ---- Q/Kå½’ä¸€åŒ–æƒé‡ï¼ˆQwen3ç‰¹æœ‰ï¼‰ ----
        attn_q_norm_tensors = []
        attn_k_norm_tensors = []
        
        if hasattr(naming, 'q_norm') and hasattr(naming, 'k_norm'):
            try:
                for i in range(nlayer):
                    q_norm_tensor = state_dict[naming.q_norm(i)].to(torch_dt_norm)
                    k_norm_tensor = state_dict[naming.k_norm(i)].to(torch_dt_norm)
                    attn_q_norm_tensors.append(q_norm_tensor)
                    attn_k_norm_tensors.append(k_norm_tensor)
                
                self._tensor_refs.extend(attn_q_norm_tensors)
                self._tensor_refs.extend(attn_k_norm_tensors)
                
                attn_q_norm_ptrs = [tensor.data_ptr() for tensor in attn_q_norm_tensors]
                attn_k_norm_ptrs = [tensor.data_ptr() for tensor in attn_k_norm_tensors]
                
                self.c_struct.attn_q_norm = (c_void_p * nlayer)(*attn_q_norm_ptrs)
                self.c_struct.attn_k_norm = (c_void_p * nlayer)(*attn_k_norm_ptrs)
                
                print(f"âœ“ å·²åŠ è½½{nlayer}å±‚çš„Q/Kå½’ä¸€åŒ–æƒé‡")
            except KeyError as e:
                print(f"âš  æœªæ‰¾åˆ°Q/Kå½’ä¸€åŒ–æƒé‡: {e}")
                # åˆ›å»ºç©ºæŒ‡é’ˆæ•°ç»„
                null_ptrs = [None for _ in range(nlayer)]
                self.c_struct.attn_q_norm = (c_void_p * nlayer)(*null_ptrs)
                self.c_struct.attn_k_norm = (c_void_p * nlayer)(*null_ptrs)
        else:
            # åˆ›å»ºç©ºæŒ‡é’ˆæ•°ç»„
            null_ptrs = [None for _ in range(nlayer)]
            self.c_struct.attn_q_norm = (c_void_p * nlayer)(*null_ptrs)
            self.c_struct.attn_k_norm = (c_void_p * nlayer)(*null_ptrs)

        # ---- QKVæŠ•å½±æƒé‡ï¼ˆåˆ†å¼€å­˜å‚¨ï¼‰ ----
        attn_q_proj_tensors = []
        attn_k_proj_tensors = []
        attn_v_proj_tensors = []
        
        for i in range(nlayer):
            q_tensor = state_dict[naming.attn_q(i)].to(torch_dt_mat)
            k_tensor = state_dict[naming.attn_k(i)].to(torch_dt_mat)
            v_tensor = state_dict[naming.attn_v(i)].to(torch_dt_mat)
            
            if not transpose_weight:
                q_tensor = q_tensor.transpose(0, 1).contiguous()
                k_tensor = k_tensor.transpose(0, 1).contiguous()
                v_tensor = v_tensor.transpose(0, 1).contiguous()
            
            attn_q_proj_tensors.append(q_tensor)
            attn_k_proj_tensors.append(k_tensor)
            attn_v_proj_tensors.append(v_tensor)

        self._tensor_refs.extend(attn_q_proj_tensors)
        self._tensor_refs.extend(attn_k_proj_tensors)
        self._tensor_refs.extend(attn_v_proj_tensors)
        
        attn_q_proj_ptrs = [tensor.data_ptr() for tensor in attn_q_proj_tensors]
        attn_k_proj_ptrs = [tensor.data_ptr() for tensor in attn_k_proj_tensors]
        attn_v_proj_ptrs = [tensor.data_ptr() for tensor in attn_v_proj_tensors]
        
        self.c_struct.attn_q_proj = (c_void_p * nlayer)(*attn_q_proj_ptrs)
        self.c_struct.attn_k_proj = (c_void_p * nlayer)(*attn_k_proj_ptrs)
        self.c_struct.attn_v_proj = (c_void_p * nlayer)(*attn_v_proj_ptrs)

        # ---- æ³¨æ„åŠ›è¾“å‡ºæƒé‡ ----
        attn_o_proj_tensors = []
        for i in range(nlayer):
            o_tensor = state_dict[naming.attn_o(i)].to(torch_dt_mat)
            if not transpose_weight:
                o_tensor = o_tensor.transpose(0, 1).contiguous()
            attn_o_proj_tensors.append(o_tensor)
        
        self._tensor_refs.extend(attn_o_proj_tensors)
        attn_o_proj_ptrs = [tensor.data_ptr() for tensor in attn_o_proj_tensors]
        self.c_struct.attn_o_proj = (c_void_p * nlayer)(*attn_o_proj_ptrs)

        # ---- FFNå½’ä¸€åŒ–æƒé‡ ----
        mlp_norm_tensors = [
            state_dict[naming.ffn_norm(i)].to(torch_dt_norm) for i in range(nlayer)
        ]
        self._tensor_refs.extend(mlp_norm_tensors)
        mlp_norm_ptrs = [tensor.data_ptr() for tensor in mlp_norm_tensors]
        self.c_struct.mlp_norm = (c_void_p * nlayer)(*mlp_norm_ptrs)

        # ---- FFNæŠ•å½±æƒé‡ï¼ˆåˆ†å¼€å­˜å‚¨ï¼‰ ----
        mlp_gate_proj_tensors = []
        mlp_up_proj_tensors = []
        mlp_down_proj_tensors = []
        
        for i in range(nlayer):
            gate_tensor = state_dict[naming.gate(i)].to(torch_dt_mat)
            up_tensor = state_dict[naming.up(i)].to(torch_dt_mat)
            down_tensor = state_dict[naming.down(i)].to(torch_dt_mat)
            
            if not transpose_weight:
                gate_tensor = gate_tensor.transpose(0, 1).contiguous()
                up_tensor = up_tensor.transpose(0, 1).contiguous()
                down_tensor = down_tensor.transpose(0, 1).contiguous()
            
            mlp_gate_proj_tensors.append(gate_tensor)
            mlp_up_proj_tensors.append(up_tensor)
            mlp_down_proj_tensors.append(down_tensor)

        self._tensor_refs.extend(mlp_gate_proj_tensors)
        self._tensor_refs.extend(mlp_up_proj_tensors)
        self._tensor_refs.extend(mlp_down_proj_tensors)
        
        mlp_gate_proj_ptrs = [tensor.data_ptr() for tensor in mlp_gate_proj_tensors]
        mlp_up_proj_ptrs = [tensor.data_ptr() for tensor in mlp_up_proj_tensors]
        mlp_down_proj_ptrs = [tensor.data_ptr() for tensor in mlp_down_proj_tensors]
        
        self.c_struct.mlp_gate_proj = (c_void_p * nlayer)(*mlp_gate_proj_ptrs)
        self.c_struct.mlp_up_proj = (c_void_p * nlayer)(*mlp_up_proj_ptrs)
        self.c_struct.mlp_down_proj = (c_void_p * nlayer)(*mlp_down_proj_ptrs)

        # ---- éªŒè¯å’Œä¿®å¤æµç¨‹ ----
        # ç¡®ä¿å¼ é‡è¿ç»­
        self.ensure_tensors_contiguous()
        
        # éªŒè¯å¼ é‡æ•°æ®
        self.validate_tensor_data()
        
        # ---- éªŒè¯å…³é”®æƒé‡ ----
        self.validate_weights()
        
    def ensure_tensors_contiguous(self):
        """ç¡®ä¿æ‰€æœ‰å¼ é‡æ˜¯è¿ç»­å†…å­˜å¸ƒå±€"""
        print("\næ£€æŸ¥å¼ é‡å†…å­˜è¿ç»­æ€§...")
        non_contiguous_count = 0
        
        for i, tensor in enumerate(self._tensor_refs):
            if not tensor.is_contiguous():
                print(f"  è­¦å‘Š: å¼ é‡ {i} ä¸è¿ç»­")
                self._tensor_refs[i] = tensor.contiguous()  # æ›¿æ¢ä¸ºè¿ç»­ç‰ˆæœ¬
                non_contiguous_count += 1
        
        if non_contiguous_count > 0:
            print(f"  å·²ä¿®å¤ {non_contiguous_count} ä¸ªä¸è¿ç»­å¼ é‡")
        else:
            print("  æ‰€æœ‰å¼ é‡å·²æ˜¯è¿ç»­å†…å­˜å¸ƒå±€")
    
    def validate_tensor_data(self):
        """éªŒè¯å¼ é‡æ•°æ®æœ‰æ•ˆæ€§"""
        print("\n==== å¼ é‡æ•°æ®éªŒè¯ ====")
        
        # æ£€æŸ¥å¸¸è§é—®é¢˜ï¼Œå¦‚NaNå’ŒInf
        has_issues = False
        for i, tensor in enumerate(self._tensor_refs[:10]):  # æ£€æŸ¥å‰10ä¸ªå¼ é‡
            # æ£€æŸ¥NaN
            if torch.isnan(tensor).any():
                print(f"  âŒ å¼ é‡ {i} åŒ…å«NaNå€¼")
                has_issues = True
            
            # æ£€æŸ¥Inf
            if torch.isinf(tensor).any():
                print(f"  âŒ å¼ é‡ {i} åŒ…å«Infå€¼")
                has_issues = True
                
            # æ£€æŸ¥æ˜¯å¦å…¨é›¶
            if (tensor == 0).all():
                print(f"  âš ï¸ å¼ é‡ {i} å…¨ä¸ºé›¶")
                
        if not has_issues:
            print("  âœ“ æ£€æŸ¥çš„å¼ é‡æ•°æ®æœ‰æ•ˆï¼Œæ— NaNæˆ–Inf")

    def validate_weights(self):
        """éªŒè¯å…³é”®æƒé‡æ˜¯å¦æ­£ç¡®åŠ è½½"""
        if not self.c_struct.input_embd:
            raise RuntimeError("è¾“å…¥åµŒå…¥æƒé‡æŒ‡é’ˆä¸ºç©º")
        if not self.c_struct.output_embd:
            raise RuntimeError("è¾“å‡ºåµŒå…¥æƒé‡æŒ‡é’ˆä¸ºç©º")
        if not self.c_struct.output_norm:
            raise RuntimeError("è¾“å‡ºå½’ä¸€åŒ–æƒé‡æŒ‡é’ˆä¸ºç©º")
        
        # æ‰“å°æƒé‡æŒ‡é’ˆä»¥ä¾¿è°ƒè¯•
        print("\n=== æƒé‡ç»“æ„ä½“éªŒè¯ ===")
        print(f"nlayer: {self.c_struct.nlayer}")
        
        # ä¿®å¤ï¼šå®‰å…¨åœ°æ‰“å°æšä¸¾å€¼
        try:
            print(f"dt_norm: {int(self.c_struct.dt_norm.value) if hasattr(self.c_struct.dt_norm, 'value') else self.c_struct.dt_norm}")
            print(f"dt_mat: {int(self.c_struct.dt_mat.value) if hasattr(self.c_struct.dt_mat, 'value') else self.c_struct.dt_mat}")
        except (ValueError, AttributeError) as e:
            print(f"dt_norm: <enum object> (cannot convert to int: {e})")
            print(f"dt_mat: <enum object> (cannot convert to int: {e})")
            
        print(f"transpose_linear_weights: {self.c_struct.transpose_linear_weights}")
        
        print(f"input_embd æŒ‡é’ˆ: {hex(self.c_struct.input_embd)}")
        print(f"output_norm æŒ‡é’ˆ: {hex(self.c_struct.output_norm)}")
        print(f"output_embd æŒ‡é’ˆ: {hex(self.c_struct.output_embd)}")
        
        # æ£€æŸ¥ç¬¬ä¸€ä¸ªå›¾å±‚çš„å…³é”®æŒ‡é’ˆ
        if self.c_struct.nlayer > 0:
            try:
                print(f"attn_norm[0] æŒ‡é’ˆ: {hex(self.c_struct.attn_norm[0])}")
                print(f"attn_q_proj[0] æŒ‡é’ˆ: {hex(self.c_struct.attn_q_proj[0])}")
                print(f"attn_k_proj[0] æŒ‡é’ˆ: {hex(self.c_struct.attn_k_proj[0])}")
                print(f"attn_v_proj[0] æŒ‡é’ˆ: {hex(self.c_struct.attn_v_proj[0])}")
                print(f"attn_o_proj[0] æŒ‡é’ˆ: {hex(self.c_struct.attn_o_proj[0])}")
                print(f"mlp_norm[0] æŒ‡é’ˆ: {hex(self.c_struct.mlp_norm[0])}")
                print(f"mlp_gate_proj[0] æŒ‡é’ˆ: {hex(self.c_struct.mlp_gate_proj[0])}")
                print(f"mlp_up_proj[0] æŒ‡é’ˆ: {hex(self.c_struct.mlp_up_proj[0])}")
                print(f"mlp_down_proj[0] æŒ‡é’ˆ: {hex(self.c_struct.mlp_down_proj[0])}")
                
                # æ‰“å°ä¸€äº›æƒé‡æ•°æ®ï¼ŒéªŒè¯å†…å®¹
                print("\næƒé‡æ•°å€¼ç¤ºä¾‹ (attn_q_proj[0]):")
                q_ptr = self.c_struct.attn_q_proj[0]
                for i, tensor in enumerate(self._tensor_refs):
                    if tensor.data_ptr() == q_ptr:
                        print(tensor.flatten()[:5].tolist())
                        break
            except Exception as e:
                print(f"æŒ‡é’ˆæ£€æŸ¥å¤±è´¥: {e}")
        
        print("=== éªŒè¯å®Œæˆ ===\n")
    
    def __getattr__(self, name):
        """ä»£ç†å±æ€§è®¿é—®åˆ°Cç»“æ„ä½“"""
        # è¿™æ ·å¯ä»¥ä¿æŒä¸åŸæœ‰ä»£ç çš„å…¼å®¹æ€§
        return getattr(self.c_struct, name)

class Qwen3BatchedTask:
    """Batched inference task for Qwen3"""
    
    def __init__(self, tasks: List[Qwen3InferTask]):
        self.tasks = tasks
        self.nreq = len(tasks)

        # Precompute fields
        token_lists = [t.tokens for t in tasks]
        self.req_lens_list = [len(toks) for toks in token_lists]
        self.req_pos_list = [t.pos for t in tasks]
        self.kv_cache_ptrs = [t.kvcache().data() for t in tasks]
        self.temperatures_list = [t.temperature for t in tasks]
        self.topks_list = [t.topk for t in tasks]
        self.topps_list = [t.topp for t in tasks]

        # Flatten token lists
        flat_tokens = [tok for toks in token_lists for tok in toks]
        self.ntok = len(flat_tokens)

        # Convert to ctypes arrays
        self.tokens = (c_uint * self.ntok)(*flat_tokens)
        self.req_lens = (c_uint * self.nreq)(*self.req_lens_list)
        self.req_pos = (c_uint * self.nreq)(*self.req_pos_list)
        self.kv_caches = (POINTER(KVCacheCStruct) * self.nreq)(*self.kv_cache_ptrs)
        self.temperatures = (c_float * self.nreq)(*self.temperatures_list)
        self.topks = (c_uint * self.nreq)(*self.topks_list)
        self.topps = (c_float * self.nreq)(*self.topps_list)

    def input_args(self):
        return (
            self.tokens,
            self.ntok,
            self.req_lens,
            self.nreq,
            self.req_pos,
            self.kv_caches,
            self.temperatures,
            self.topks,
            self.topps,
        )


class QwenForCausalLM:
    """Qwen3 model for causal language modeling - FIXED VERSION"""
    
    def __init__(
        self, model_dir_path, device=DeviceType.DEVICE_TYPE_CPU, ndev=1, max_tokens=None
    ):
        def load_all_safetensors_from_dir(dir_path_: str):
            tensors_ = {}
            dir_path_ = Path(dir_path_)
            for file in sorted(dir_path_.glob("*.safetensors")):
                data_ = safetensors.safe_open(file, "pt")
                for name_ in data_.keys():
                    tensors_[name_] = data_.get_tensor(name_)
            return tensors_

        print("Loading Qwen3 model weights to host...")
        load_start_time = time.time()

        with open(os.path.join(model_dir_path, "config.json"), "r") as f:
            config = json.load(f)
            self.config = config
            
        eos_token_id = self.config.get("eos_token_id", 2)
        self.eos_token_id = (
            [eos_token_id] if type(eos_token_id) == int else eos_token_id
        )
        
        transpose_weight = (
            device != DeviceType.DEVICE_TYPE_ASCEND
        )

        # Load state dict
        if any(file.suffix == ".safetensors" for file in Path(model_dir_path).iterdir()):
            state_dict = load_all_safetensors_from_dir(model_dir_path)
        else:
            state_dict = torch.load(
                os.path.join(model_dir_path, "pytorch_model.bin"),
                weights_only=True,
                map_location="cpu",
            )

        # Determine naming scheme
        if Qwen3WeightsNaming.match(state_dict):
            print("âœ“ Using Qwen3WeightsNaming (with Q/K normalization)")
            # Create metadata and weights
            self.meta = Qwen3MetaFromConfig(config, max_tokens=max_tokens)
            self.weights = Qwen3WeightsImpl(
                self.meta,
                Qwen3WeightsNaming(),
                state_dict,
                ndev=ndev,
                transpose_weight=transpose_weight,
            )
            # Load tokenizer
            self.tokenizer = transformers.AutoTokenizer.from_pretrained(
                model_dir_path, trust_remote_code=True
            )
            if self.tokenizer.bos_token_id is None:
                print(f"ä¿®å¤BOS token: è®¾ç½®åˆ†è¯å™¨BOS token ID = {self.meta.bos_token}")
                self.tokenizer.bos_token_id = self.meta.bos_token
                
                # å¦‚æœå¯èƒ½ï¼Œä¹Ÿè®¾ç½®ç›¸åº”çš„æ–‡æœ¬è¡¨ç¤º
                if hasattr(self.tokenizer, '_tokenizer'):
                    try:
                        bos_text = self.tokenizer._tokenizer.id_to_token(self.meta.bos_token)
                        if bos_text:
                            self.tokenizer.bos_token = bos_text
                            print(f"  è®¾ç½®BOS tokenæ–‡æœ¬è¡¨ç¤º: '{bos_text}'")
                    except:
                        pass
    
        elif LlamaWeightsNaming.match(state_dict):
            print("âš  Using LlamaWeightsNaming (fallback, no Q/K normalization)")
        else:
            raise ValueError("Unsupported weight naming scheme")

        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        load_end_time = time.time()
        print(f"Weight loading time: {load_end_time - load_start_time:.3f}s")

        print(f"Creating Qwen3 model on {ndev} devices...")
        load_start_time = time.time()
        
        # éªŒè¯å¤šè®¾å¤‡åˆ†å¸ƒ
        if ndev > 1:
            self.validate_multi_device_setup(ndev)
        
        # æ£€æŸ¥ç»“æ„ä½“å¸ƒå±€
        self.check_struct_layout()
        
        # éªŒè¯æ‰€æœ‰æŒ‡é’ˆ
        self.debug_pointers_before_call()
        
        # æ£€æŸ¥å†…å­˜çŠ¶æ€
        self.diagnose_memory_issues()
        
        # åˆ›å»ºCå…¼å®¹çš„è®¾å¤‡IDæ•°ç»„
        dev_ids_arr = (c_int * ndev)(*[i for i in range(ndev)])

        # ç¡®ä¿byrefæ­£ç¡®åº”ç”¨åˆ°ç»“æ„ä½“
        meta_ptr = ctypes.byref(self.meta)
        weights_ptr = ctypes.byref(self.weights.c_struct)
    
        try:
            print(f"\n==== å¼€å§‹åˆ›å»ºæ¨¡å‹ ====")
            print(f"ä¼ é€’å‚æ•°:")
            print(f"  meta: {ctypes.addressof(self.meta):#x}")
            print(f"  weights.c_struct: {ctypes.addressof(self.weights.c_struct):#x}")
            print(f"  device: {device}")
            print(f"  ndev: {ndev}")
            print(f"  dev_ids: {[dev_ids_arr[i] for i in range(ndev)]}")
            
            # è¯¦ç»†æ£€æŸ¥metaç»“æ„ä½“å€¼
            print("\nmetaå…³é”®å­—æ®µ:")
            print(f"  nlayer: {self.meta.nlayer}")
            print(f"  d: {self.meta.d}")
            print(f"  nh: {self.meta.nh}")
            print(f"  nkvh: {self.meta.nkvh}")
           
            # ä¿®å¤ï¼šå®‰å…¨åœ°æ‰“å°dt_logits
            try:
                if hasattr(self.meta.dt_logits, 'value'):
                    print(f"  dt_logits: {int(self.meta.dt_logits.value)}")
                else:
                    print(f"  dt_logits: {self.meta.dt_logits}")
            except (ValueError, AttributeError):
                print(f"  dt_logits: <enum object>")
            
            # è¯¦ç»†æ£€æŸ¥weightsç»“æ„ä½“å€¼
            print("\nweightså…³é”®å­—æ®µ:")
            print(f"  nlayer: {self.weights.c_struct.nlayer}")
            
            # ä¿®å¤ï¼šå®‰å…¨åœ°æ‰“å°dt_matå’Œdt_norm
            try:
                if hasattr(self.weights.c_struct.dt_mat, 'value'):
                    print(f"  dt_mat: {int(self.weights.c_struct.dt_mat.value)}")
                else:
                    print(f"  dt_mat: {self.weights.c_struct.dt_mat}")
            except (ValueError, AttributeError):
                print(f"  dt_mat: <enum object>")
                
            try:
                if hasattr(self.weights.c_struct.dt_norm, 'value'):
                    print(f"  dt_norm: {int(self.weights.c_struct.dt_norm.value)}")
                else:
                    print(f"  dt_norm: {self.weights.c_struct.dt_norm}")
            except (ValueError, AttributeError):
                print(f"  dt_norm: <enum object>")
            
            # è°ƒç”¨Cå‡½æ•°
            self.model_instance = create_qwen3_model(
                meta_ptr,  # ä½¿ç”¨æ˜ç¡®çš„å˜é‡
                weights_ptr,  # ä½¿ç”¨æ˜ç¡®çš„å˜é‡  
                device,
                ndev,
                dev_ids_arr  # ä½¿ç”¨å˜é‡è€Œéç›´æ¥æ„é€ 
            )
            
            # æ£€æŸ¥è¿”å›å€¼
            if not self.model_instance:
                raise RuntimeError("åˆ›å»ºæ¨¡å‹å¤±è´¥: è¿”å›ç©ºæŒ‡é’ˆ")
                
            print(f"âœ“ æ¨¡å‹å®ä¾‹: {self.model_instance}")
        except Exception as e:
            print(f"âœ— æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise

        load_end_time = time.time()
        print(f"Model creation time: {load_end_time - load_start_time:.3f}s")
        if self.model_instance is None:
            raise RuntimeError("Model instance is None after creation")

    def validate_multi_device_setup(self, ndev):
        """éªŒè¯å¤šè®¾å¤‡è®¾ç½®çš„å…¼å®¹æ€§"""
        print(f"\n==== å¤šè®¾å¤‡è®¾ç½®éªŒè¯ ({ndev}) ====")
        # éªŒè¯æ¨¡å‹ç»´åº¦æ˜¯å¦ä¸è®¾å¤‡æ•°é‡å…¼å®¹
        if self.meta.nh % ndev != 0:
            print(f"âš ï¸ æ³¨æ„åŠ›å¤´æ•° {self.meta.nh} ä¸èƒ½è¢«è®¾å¤‡æ•° {ndev} æ•´é™¤")
        if self.meta.nkvh % ndev != 0:
            print(f"âš ï¸ KVå¤´æ•° {self.meta.nkvh} ä¸èƒ½è¢«è®¾å¤‡æ•° {ndev} æ•´é™¤")
        if self.meta.di % ndev != 0:
            print(f"âš ï¸ ä¸­é—´ç»´åº¦ {self.meta.di} ä¸èƒ½è¢«è®¾å¤‡æ•° {ndev} æ•´é™¤")

    def check_struct_layout(self):
        """æ£€æŸ¥ç»“æ„ä½“å†…å­˜å¸ƒå±€ä¸C++ä¸€è‡´æ€§"""
        print("\n==== ç»“æ„ä½“å†…å­˜å¸ƒå±€éªŒè¯ ====")
        
        # æ‰“å°ç»“æ„ä½“å¤§å°
        meta_size = ctypes.sizeof(self.meta)
        weights_size = ctypes.sizeof(self.weights.c_struct)
        print(f"Metaç»“æ„ä½“å¤§å°: {meta_size}å­—èŠ‚")
        print(f"Weightsç»“æ„ä½“å¤§å°: {weights_size}å­—èŠ‚")
        
        # éªŒè¯ç»“æ„ä½“å­—æ®µåç§»é‡
        meta_fields = Qwen3MetaCStruct._fields_
        weights_fields = Qwen3WeightsCStruct._fields_
        
        print("\nMetaç»“æ„ä½“å­—æ®µåç§»:")
        offset = 0
        for field_name, field_type in meta_fields:
            field_size = ctypes.sizeof(field_type)
            print(f"  {field_name}: åç§»={offset}, å¤§å°={field_size}")
            offset += field_size
            
        print("\nWeightsç»“æ„ä½“å­—æ®µåç§»:")
        offset = 0
        for field_name, field_type in weights_fields:
            if hasattr(field_type, '_type_'):  # æŒ‡é’ˆæ•°ç»„
                field_size = ctypes.sizeof(field_type)
            else:
                field_size = ctypes.sizeof(field_type)
            print(f"  {field_name}: åç§»={offset}, å¤§å°={field_size}")
            offset += field_size

    def debug_pointers_before_call(self):
        """éªŒè¯æ‰€æœ‰å…³é”®æŒ‡é’ˆåœ¨è°ƒç”¨C++å‰çš„æœ‰æ•ˆæ€§"""
        print("\n==== C++è°ƒç”¨å‰æŒ‡é’ˆéªŒè¯ ====")
        
        # 1. åŸºæœ¬ç»“æ„ä½“
        print(f"metaç»“æ„ä½“åœ°å€: {ctypes.addressof(self.meta):#x}")
        print(f"weights.c_structç»“æ„ä½“åœ°å€: {ctypes.addressof(self.weights.c_struct):#x}")
        
        # 2. åŸºæœ¬å­—æ®µ
        print(f"nlayer: Python={self.meta.nlayer}, Cç»“æ„ä½“={self.weights.c_struct.nlayer}")
        
        # 3. æƒé‡æŒ‡é’ˆ - æ£€æŸ¥ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ª
        print("\nå…³é”®æƒé‡æŒ‡é’ˆéªŒè¯:")
        nlayer = self.meta.nlayer
        
        # è¾“å…¥è¾“å‡ºæƒé‡
        print(f"input_embd: {self.weights.c_struct.input_embd:#x}")
        print(f"output_embd: {self.weights.c_struct.output_embd:#x}")
        print(f"output_norm: {self.weights.c_struct.output_norm:#x}")
        
        # éªŒè¯æ•°ç»„æŒ‡é’ˆ
        print(f"\nå±‚æƒé‡æ•°ç»„æŒ‡é’ˆ:")
        print(f"attn_normæ•°ç»„: {ctypes.addressof(self.weights.c_struct.attn_norm.contents) if self.weights.c_struct.attn_norm else 'NULL'}")
        print(f"attn_q_projæ•°ç»„: {ctypes.addressof(self.weights.c_struct.attn_q_proj.contents) if self.weights.c_struct.attn_q_proj else 'NULL'}")
        
        # éªŒè¯ç¬¬0å±‚å’Œæœ€åä¸€å±‚çš„æŒ‡é’ˆ
        print(f"\nç¬¬ä¸€å±‚(0)æƒé‡æŒ‡é’ˆ:")
        print(f"  attn_norm[0]: {self.weights.c_struct.attn_norm[0]:#x}")
        print(f"  attn_q_proj[0]: {self.weights.c_struct.attn_q_proj[0]:#x}")
        print(f"  attn_k_proj[0]: {self.weights.c_struct.attn_k_proj[0]:#x}")
        print(f"  attn_v_proj[0]: {self.weights.c_struct.attn_v_proj[0]:#x}")
        
        print(f"\næœ€åä¸€å±‚({nlayer-1})æƒé‡æŒ‡é’ˆ:")
        print(f"  attn_norm[{nlayer-1}]: {self.weights.c_struct.attn_norm[nlayer-1]:#x}")
        print(f"  attn_q_proj[{nlayer-1}]: {self.weights.c_struct.attn_q_proj[nlayer-1]:#x}")

    def diagnose_memory_issues(self):
        """è¯Šæ–­æ½œåœ¨çš„å†…å­˜é—®é¢˜"""
        import gc
        import sys
        
        print("\n==== å†…å­˜è¯Šæ–­ ====")
        
        # å¼ºåˆ¶GC
        gc.collect()
        print(f"Pythonå¼•ç”¨è®¡æ•°: {sys.getrefcount(self) - 1}")  # -1æ’é™¤å½“å‰å‡½æ•°è°ƒç”¨
        
        # æ£€æŸ¥å¼ é‡å¼•ç”¨
        print(f"æŒæœ‰çš„å¼ é‡å¼•ç”¨æ•°: {len(self.weights._tensor_refs)}")
        print(f"  ç¬¬ä¸€ä¸ªå¼ é‡ä¿¡æ¯: {self.weights._tensor_refs[0].shape}, {self.weights._tensor_refs[0].dtype}")
        print(f"  æœ€åä¸€ä¸ªå¼ é‡ä¿¡æ¯: {self.weights._tensor_refs[-1].shape}, {self.weights._tensor_refs[-1].dtype}")
        
        # æ£€æŸ¥è®¾å¤‡å†…å­˜
        try:
            if torch.cuda.is_available():
                print(f"CUDAå†…å­˜åˆ†é…: {torch.cuda.memory_allocated()/1024**2:.2f} MB")
                print(f"CUDAå†…å­˜ç¼“å­˜: {torch.cuda.memory_reserved()/1024**2:.2f} MB")
        except:
            print("æœªæ‰¾åˆ°CUDAæˆ–torch")

    def max_context_len(self):
        return self.meta.dctx

    def create_kv_cache(self):
        # FIXED: Use proper Qwen3 KV cache API
        return create_qwen3_kv_cache(self.model_instance)

    def drop_kv_cache(self, kv_cache):
        # FIXED: Use proper Qwen3 KV cache API
        drop_qwen3_kv_cache(self.model_instance, kv_cache)

    def batch_infer_one_round(self, tasks: List[Qwen3InferTask]):
        output = (c_uint * len(tasks))()
        
        # ä½¿ç”¨Qwen3BatchedTaskç±»æ¥å¤„ç†æ‰¹å¤„ç†
        batch_inputs = Qwen3BatchedTask(tasks)


        # éªŒè¯è¾“å…¥å‚æ•°
        if batch_inputs.ntok == 0:
            raise ValueError("æ²¡æœ‰tokenséœ€è¦å¤„ç†")
        if batch_inputs.nreq == 0:
            raise ValueError("æ²¡æœ‰è¯·æ±‚éœ€è¦å¤„ç†")
        
        try:
            # ä½¿ç”¨batch_inputsä¸­çš„æ•°ç»„
            # print("ğŸš€ Calling infer_qwen3_batch...")
            infer_qwen3_batch(
                self.model_instance,
                *batch_inputs.input_args(),
                output,
            )
            # print("âœ… infer_qwen3_batch completed")
            
            # éªŒè¯è¾“å‡ºtoken
            # for i, token in enumerate(list(output)):
            #     print(f"  Output token[{i}]: {token}")
            #     if token >= self.meta.dvoc:
            #         print(f"    âš  Invalid: exceeds vocab_size {self.meta.dvoc}")
            #     if token < 0:
            #         print(f"    âš  Invalid: negative token")
                    
        except Exception as e:
            # print(f"âŒ C++ inference failed: {e}")
            import traceback
            traceback.print_exc()
            raise
            
        return list(output)
    def generate(self, input_content, max_steps, topp_=0.8, topk_=50, temperature_=0.7):
        # Apply chat template if available
        if hasattr(self.tokenizer, 'chat_template') and self.tokenizer.chat_template:
            input_content = self.tokenizer.apply_chat_template(
                conversation=[{"role": "user", "content": input_content}],
                add_generation_prompt=True,
                tokenize=False,
            )
        
        # print(input_content, end="", flush=True)
        tokens = self.tokenizer.encode(input_content)

            
        infer_task = Qwen3InferTask(
            tokens=tokens,
            position=0,
            temperature=temperature_,
            topk=topk_,
            topp=topp_,
            end_tokens=self.eos_token_id,
            max_tokens=int(self.meta.dctx),
            task_id=0
        )

        infer_task.bind_kvcache(Qwen3KVCache(self))
        # éªŒè¯æ¨¡å‹å®ä¾‹çŠ¶æ€ - FIXED
        # print(f"ğŸ” Model instance validation:")
        # try:
        #     model_ptr_addr = ctypes.addressof(self.model_instance.contents) if self.model_instance else 0
        #     print(f"    Model instance ptr: {hex(model_ptr_addr)}")
        # except:
        #     # é™çº§å¤„ç†
        #     print(f"    Model instance: {self.model_instance is not None}")
            
        # if self.model_instance is None:
        #     raise RuntimeError("âŒ Model instance is null before inference")


        steps = 0
        total_time = 0
        output_content = ""
        print("ğŸš€ Starting generation:")
        for step_i in range(max_steps):
            start_time = time.time()
            output_tokens = self.batch_infer_one_round([infer_task])
            end_time = time.time()
            steps += 1
            output_token = output_tokens[0]
            try:
                output_str = self.tokenizer.decode([output_token], skip_special_tokens=False)
                print(f"{output_str}")
            except Exception as e:
                print(f"  âš  Decode failed: {e}")
                output_str = self.tokenizer._tokenizer.id_to_token(output_token)
                if output_str is None:
                    output_str = f"[UNK_{output_token}]"
                else:
                    output_str = output_str.replace("â–", " ").replace("<0x0A>", "\n")
            
            output_content += output_str
            print(output_str, end="", flush=True)
            
            if output_tokens[0] in self.eos_token_id:
                break
                
            infer_task.next(output_tokens[0])

            if step_i > 0:
                total_time += end_time - start_time

        print("\n")
        avg_time = total_time * 1000 / (steps - 1) if steps > 1 else 0
        print(f"Time per step: {avg_time:.3f}ms")

        try:
            infer_task._kv_cache.drop()
        except AttributeError:
            # å¦‚æœdropæ–¹æ³•æœ‰é—®é¢˜ï¼Œè·³è¿‡æ¸…ç†
            print("    âš  KV cache cleanup skipped (method issue)")
        except Exception as e:
            print(f"    âš  KV cache cleanup failed: {e}")

        return output_content, avg_time

    def destroy_model_instance(self):
        # FIXED: Use proper Qwen3 model destruction API
        destroy_qwen3_model(self.model_instance)
        print("Qwen3 Model destroyed")

    def diagnose_cpp_computation(self):
        """è¯Šæ–­C++æ¨ç†å¼•æ“çš„è®¡ç®—æ­£ç¡®æ€§"""
        
        print(f"\n{'='*60}")
        print("ğŸ”¬ C++ COMPUTATION DIAGNOSIS")
        print(f"{'='*60}")
        
        # 1. æµ‹è¯•å›ºå®šè¾“å…¥çš„ä¸€è‡´æ€§
        print("\n1ï¸âƒ£ Testing computation consistency with fixed inputs:")
        
        # ä½¿ç”¨éå¸¸ç®€å•çš„è¾“å…¥
        simple_tokens = [3, 28, 1]  # BOS, simple tokens
        
        results = []
        for i in range(3):  # è¿è¡Œ3æ¬¡ç›¸åŒçš„æ¨ç†
            print(f"\n  Run {i+1}/3:")
            
            task = Qwen3InferTask(
                tokens=simple_tokens,
                position=0,
                temperature=0.0,  # å®Œå…¨ç¡®å®šæ€§
                topk=1,           # åªå–æ¦‚ç‡æœ€é«˜çš„token
                topp=1.0,
                end_tokens=self.eos_token_id,
                max_tokens=int(self.meta.dctx),
                task_id=0
            )
            task.bind_kvcache(Qwen3KVCache(self))
            
            # æ‰§è¡Œæ¨ç†
            output_tokens = self.batch_infer_one_round([task])
            output_token = output_tokens[0]
            
            print(f"    Input tokens: {simple_tokens}")
            print(f"    Output token: {output_token}")
            
            results.append(output_token)
                    # FIXED: ä½¿ç”¨try-exceptæ¥å¤„ç†KVç¼“å­˜æ¸…ç†
            try:
                task._kv_cache.drop()
            except AttributeError:
                # å¦‚æœdropæ–¹æ³•æœ‰é—®é¢˜ï¼Œè·³è¿‡æ¸…ç†
                print("    âš  KV cache cleanup skipped (method issue)")
            except Exception as e:
                print(f"    âš  KV cache cleanup failed: {e}")
        
        # æ£€æŸ¥ä¸€è‡´æ€§
        if len(set(results)) == 1:
            print(f"  âœ… PASS: All runs produced same result: {results[0]}")
        else:
            print(f"  âŒ FAIL: Inconsistent results: {results}")
            print("    This indicates non-deterministic computation or memory corruption")
        
        # 2. æµ‹è¯•ä¸åŒtemperatureçš„å½±å“
        print("\n2ï¸âƒ£ Testing temperature parameter effect:")
        
        temps = [0.0, 0.5, 1.0]
        temp_results = {}
        
        for temp in temps:
            task = Qwen3InferTask(
                tokens=simple_tokens,
                position=0,
                temperature=temp,
                topk=50,
                topp=0.8,
                end_tokens=self.eos_token_id,
                max_tokens=int(self.meta.dctx),
                task_id=0
            )
            task.bind_kvcache(Qwen3KVCache(self))
            
            # è¿è¡Œå¤šæ¬¡è·å–åˆ†å¸ƒ
            temp_outputs = []
            for _ in range(5):
                output_tokens = self.batch_infer_one_round([task])
                temp_outputs.append(output_tokens[0])
                task.next(output_tokens[0])  # æ›´æ–°çŠ¶æ€ä»¥ä¾¿ä¸‹æ¬¡æ¨ç†
            
            temp_results[temp] = temp_outputs
            try:
                task._kv_cache.drop()
            except AttributeError:
                # å¦‚æœdropæ–¹æ³•æœ‰é—®é¢˜ï¼Œè·³è¿‡æ¸…ç†
                print("    âš  KV cache cleanup skipped (method issue)")
            except Exception as e:
                print(f"    âš  KV cache cleanup failed: {e}")

            unique_outputs = len(set(temp_outputs))
            print(f"    temp={temp}: outputs={temp_outputs}, unique={unique_outputs}")
        
        # éªŒè¯temperature=0.0åº”è¯¥å®Œå…¨ç¡®å®š
        if len(set(temp_results[0.0])) == 1:
            print("  âœ… PASS: Temperature=0.0 produces deterministic output")
        else:
            print("  âŒ FAIL: Temperature=0.0 should be deterministic")
        
        # 3. æµ‹è¯•è¾“å…¥é•¿åº¦å¯¹è¾“å‡ºçš„å½±å“
        print("\n3ï¸âƒ£ Testing input length effect:")
        
        test_inputs = [
            [1],           # 1 token
            [1, 2],        # 2 tokens  
            [1, 2, 3],     # 3 tokens
            [1, 2, 3, 4],  # 4 tokens
        ]
        
        length_results = {}
        for tokens in test_inputs:
            task = Qwen3InferTask(
                tokens=tokens,
                position=0,
                temperature=0.0,
                topk=1,
                topp=1.0,
                end_tokens=self.eos_token_id,
                max_tokens=int(self.meta.dctx),
                task_id=0
            )
            task.bind_kvcache(Qwen3KVCache(self))
            
            output_tokens = self.batch_infer_one_round([task])
            length_results[len(tokens)] = output_tokens[0]
            
            print(f"    Input length {len(tokens)}: {tokens} -> {output_tokens[0]}")
            try:
                task._kv_cache.drop()
            except AttributeError:
                # å¦‚æœdropæ–¹æ³•æœ‰é—®é¢˜ï¼Œè·³è¿‡æ¸…ç†
                print("    âš  KV cache cleanup skipped (method issue)")
            except Exception as e:
                print(f"    âš  KV cache cleanup failed: {e}")

        # 4. æµ‹è¯•KVç¼“å­˜çŠ¶æ€çš„å½±å“
        print("\n4ï¸âƒ£ Testing KV cache state impact:")
        
        # ç¬¬ä¸€æ¬¡æ¨ç†
        task1 = Qwen3InferTask(
            tokens=[1, 2, 3],
            position=0,
            temperature=0.0,
            topk=1,
            topp=1.0,
            end_tokens=self.eos_token_id,
            max_tokens=int(self.meta.dctx),
            task_id=0
        )
        task1.bind_kvcache(Qwen3KVCache(self))
        
        output1 = self.batch_infer_one_round([task1])[0]
        print(f"    Fresh KV cache: [1,2,3] -> {output1}")
        
        # ç»§ç»­ç”¨ç›¸åŒçš„KVç¼“å­˜æ¨ç†ä¸‹ä¸€ä¸ªtoken
        task1.next(output1)
        output2 = self.batch_infer_one_round([task1])[0]
        print(f"    Continued KV cache: append {output1} -> {output2}")
        
        # é‡æ–°å¼€å§‹ï¼Œä½†ç”¨ä¸åŒçš„æ–¹å¼
        task2 = Qwen3InferTask(
            tokens=[1, 2, 3, output1],
            position=0,
            temperature=0.0,
            topk=1,
            topp=1.0,
            end_tokens=self.eos_token_id,
            max_tokens=int(self.meta.dctx),
            task_id=0
        )
        task2.bind_kvcache(Qwen3KVCache(self))
        
        output3 = self.batch_infer_one_round([task2])[0]
        print(f"    Fresh KV cache: [1,2,3,{output1}] -> {output3}")
        
        if output2 == output3:
            print("  âœ… PASS: KV cache state consistency maintained")
        else:
            print("  âŒ FAIL: KV cache state inconsistent")
            print(f"         Continued: {output2}, Fresh: {output3}")
        
        task1._kv_cache.drop()
        task2._kv_cache.drop()
        
        # 5. æµ‹è¯•è¾¹ç•Œæƒ…å†µ
        print("\n5ï¸âƒ£ Testing edge cases:")
        
        # æµ‹è¯•vocabè¾¹ç•Œé™„è¿‘çš„token
        edge_tokens = [0, 1, self.meta.dvoc-2, self.meta.dvoc-1]  # é¿å…æ— æ•ˆtoken
        
        for token in edge_tokens:
            if 0 <= token < self.meta.dvoc:
                task = Qwen3InferTask(
                    tokens=[token],
                    position=0,
                    temperature=0.0,
                    topk=1,
                    topp=1.0,
                    end_tokens=self.eos_token_id,
                    max_tokens=int(self.meta.dctx),
                    task_id=0
                )
                task.bind_kvcache(Qwen3KVCache(self))
                
                try:
                    output = self.batch_infer_one_round([task])[0]
                    print(f"    Edge token {token} -> {output}")
                    
                    if 0 <= output < self.meta.dvoc:
                        print(f"      âœ… Output {output} in valid range")
                    else:
                        print(f"      âŒ Output {output} out of range [0, {self.meta.dvoc})")
                        
                except Exception as e:
                    print(f"      âŒ Error with token {token}: {e}")
                finally:
                    try:
                        task._kv_cache.drop()
                    except AttributeError:
                        # å¦‚æœdropæ–¹æ³•æœ‰é—®é¢˜ï¼Œè·³è¿‡æ¸…ç†
                        print("    âš  KV cache cleanup skipped (method issue)")
                    except Exception as e:
                        print(f"    âš  KV cache cleanup failed: {e}")

        print(f"\n{'='*60}")
        print("ğŸ”¬ DIAGNOSIS COMPLETE")
        print(f"{'='*60}")
    
    def generate_simple(self, input_content, max_steps, topp_=0.8, topk_=50, temperature_=0.7):
        """ä¸ä½¿ç”¨chat templateçš„ç®€å•ç”Ÿæˆ"""
        print(f"\nSimple generation: '{input_content}'", end="", flush=True)
        tokens = self.tokenizer.encode(input_content)
    
        print(f"\nInput tokens: {tokens}")
        
        infer_task = Qwen3InferTask(
            tokens=tokens,
            position=0,
            temperature=temperature_,
            topk=topk_,
            topp=topp_,
            end_tokens=self.eos_token_id,
            max_tokens=int(self.meta.dctx),
            task_id=0
        )
    
        infer_task.bind_kvcache(Qwen3KVCache(self))
    
        output_content = ""
        for step_i in range(max_steps):
            output_tokens = self.batch_infer_one_round([infer_task])
            output_token = output_tokens[0]
            
            print(f" -> {output_token}", end="")
            
            if output_token >= self.meta.dvoc or output_token < 0:
                print(f" (INVALID)")
                break
            
            try:
                output_str = self.tokenizer.decode([output_token], skip_special_tokens=False)
            except Exception:
                output_str = f"[UNK_{output_token}]"
            
            output_content += output_str
            print(f"('{output_str}')", end="", flush=True)
            
            if output_token in self.eos_token_id:
                break
                
            infer_task.next(output_token)
    
        print(f"\nFinal output: '{output_content}'")
        try:
            infer_task._kv_cache.drop()
        except AttributeError:
            # å¦‚æœdropæ–¹æ³•æœ‰é—®é¢˜ï¼Œè·³è¿‡æ¸…ç†
            print("    âš  KV cache cleanup skipped (method issue)")
        except Exception as e:
            print(f"    âš  KV cache cleanup failed: {e}")
        return output_content






def test_basic_functionality(model):
    """æµ‹è¯•åŸºæœ¬åŠŸèƒ½ï¼Œæ’é™¤å¤æ‚é—®é¢˜å½±å“"""
    # 1. æµ‹è¯•éå¸¸ç®€å•çš„è¾“å…¥
    print("æµ‹è¯•å•ä¸ªtokenè¾“å…¥:")
    model.generate_simple("ä½ ", 5, temperature_=0.0)
    
    # 2. æµ‹è¯•ä¸åŒè¯­è¨€
    print("\næµ‹è¯•è‹±æ–‡è¾“å…¥:")
    model.generate_simple("Hello", 5, temperature_=0.0)
    
    # 3. æµ‹è¯•ä¸ä½¿ç”¨KVç¼“å­˜çš„æ¨ç†
    print("\næµ‹è¯•æ— KVç¼“å­˜æ¨ç†:")
    tokens = model.tokenizer.encode("æµ‹è¯•")
    output = []
    
    for _ in range(5):
        infer_task = Qwen3InferTask(
            tokens=tokens,
            position=0,  # å§‹ç»ˆä»å¤´å¼€å§‹
            temperature=0.0,
            topk=1,
            topp=1.0,
            end_tokens=[model.meta.end_token],
            max_tokens=int(model.meta.dctx),
            task_id=0
        )
        # æ¯æ¬¡éƒ½åˆ›å»ºæ–°ç¼“å­˜
        infer_task.bind_kvcache(Qwen3KVCache(model))
        out_token = model.batch_infer_one_round([infer_task])[0]
        output.append(out_token)
        # æ›´æ–°è¾“å…¥ï¼Œä½†ä¸ä½¿ç”¨ç¼“å­˜çŠ¶æ€
        tokens = tokens + [out_token]
        # æ¸…ç†ç¼“å­˜
        infer_task._kv_cache.drop()
    
    print(f"ç”Ÿæˆç»“æœ: '{model.tokenizer.decode(output)}'")

def fix_tokenizer_model_mismatch(model, model_dir_path):
    """æ£€æŸ¥å¹¶å°è¯•ä¿®å¤åˆ†è¯å™¨ä¸æ¨¡å‹ä¹‹é—´çš„ä¸åŒ¹é…é—®é¢˜"""
    import os
    
    print("\n==== åˆ†è¯å™¨ä¸æ¨¡å‹åŒ¹é…æ£€æŸ¥ ====")
    
    # 1. æ£€æŸ¥è¯æ±‡è¡¨å¤§å°
    model_vocab_size = model.meta.dvoc
    tokenizer_vocab_size = len(model.tokenizer.get_vocab())
    
    print(f"æ¨¡å‹è¯æ±‡è¡¨å¤§å°: {model_vocab_size}")
    print(f"åˆ†è¯å™¨è¯æ±‡è¡¨å¤§å°: {tokenizer_vocab_size}")
    
    if model_vocab_size != tokenizer_vocab_size:
        print(f"âš ï¸ è­¦å‘Š: è¯æ±‡è¡¨å¤§å°ä¸åŒ¹é…! æ¨¡å‹: {model_vocab_size}, åˆ†è¯å™¨: {tokenizer_vocab_size}")
    
    # 2. æ£€æŸ¥ç‰¹æ®Štoken
    print(f"æ¨¡å‹BOS token ID: {model.meta.bos_token}")
    print(f"åˆ†è¯å™¨BOS token ID: {model.tokenizer.bos_token_id}")
    print(f"æ¨¡å‹EOS token ID: {model.meta.end_token}")
    print(f"åˆ†è¯å™¨EOS token ID: {model.tokenizer.eos_token_id}")
    
    if model.meta.bos_token != model.tokenizer.bos_token_id or model.meta.end_token != model.tokenizer.eos_token_id:
        print(f"âš ï¸ è­¦å‘Š: ç‰¹æ®Štokenä¸åŒ¹é…!")
    
    # 3. æ£€æŸ¥åˆ†è¯å™¨æ–‡ä»¶
    tokenizer_files = [f for f in os.listdir(model_dir_path) 
                      if f in ['tokenizer.json', 'tokenizer_config.json', 'vocab.txt', 'tokenizer.model']]
    print(f"åˆ†è¯å™¨ç›¸å…³æ–‡ä»¶: {tokenizer_files}")
    
    # 4. æµ‹è¯•åŸºæœ¬è¯æ±‡çš„ç¼–è§£ç 
    test_words = ["ä½ å¥½", "Hello", "world", "æµ‹è¯•"]
    print("\nåŸºæœ¬è¯æ±‡ç¼–è§£ç æµ‹è¯•:")
    for text in test_words:
        tokens = model.tokenizer.encode(text)
        decoded = model.tokenizer.decode(tokens)
        print(f"'{text}' -> {tokens} -> '{decoded}'")
    
    # 5. æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„è¯æ±‡è¡¨å¤§å°
    try:
        import json
        with open(os.path.join(model_dir_path, "config.json"), "r") as f:
            config = json.load(f)
            config_vocab_size = config.get('vocab_size')
            print(f"é…ç½®æ–‡ä»¶ä¸­çš„è¯æ±‡è¡¨å¤§å°: {config_vocab_size}")
            
            if config_vocab_size != model_vocab_size:
                print(f"âš ï¸ è­¦å‘Š: é…ç½®æ–‡ä»¶è¯æ±‡è¡¨å¤§å°ä¸æ¨¡å‹ä¸åŒ¹é…!")
    except Exception as e:
        print(f"æ— æ³•è¯»å–é…ç½®æ–‡ä»¶: {e}")
    
    print("\n==== è§£å†³æ–¹æ¡ˆå»ºè®® ====")
    if model_vocab_size != tokenizer_vocab_size:
        print("1. ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„åˆ†è¯å™¨æ–‡ä»¶ï¼Œä¸æ¨¡å‹æƒé‡é…å¥—")
        print("2. å°è¯•ä»åŸå§‹Hugging Faceä»“åº“é‡æ–°ä¸‹è½½å®Œæ•´æ¨¡å‹")
        print("3. æ£€æŸ¥è½¬æ¢è¿‡ç¨‹æ˜¯å¦æ­£ç¡®ä¿ç•™äº†æ‰€æœ‰åˆ†è¯å™¨æ–‡ä»¶")
    
    return tokenizer_vocab_size == model_vocab_size

def verify_model_weights(model_dir_path):
    # 1. æ£€æŸ¥æƒé‡æ–‡ä»¶çš„å®Œæ•´æ€§
    import os
    safetensors_files = [f for f in os.listdir(model_dir_path) if f.endswith('.safetensors')]
    print(f"å‘ç°æƒé‡æ–‡ä»¶: {safetensors_files}")
    
    # 2. éªŒè¯æƒé‡æ–‡ä»¶å¤§å°æ˜¯å¦æ­£ç¡®
    total_size = sum(os.path.getsize(os.path.join(model_dir_path, f)) for f in safetensors_files)
    print(f"æƒé‡æ€»å¤§å°: {total_size / (1024**3):.2f} GB")
    
    # 3. æ£€æŸ¥é…ç½®æ–‡ä»¶
    with open(os.path.join(model_dir_path, "config.json"), "r") as f:
        import json
        config = json.load(f)
        print(f"æ¨¡å‹ç±»å‹: {config.get('model_type')}")
        print(f"é…ç½®æ–‡ä»¶ä¸­çš„è¯æ±‡è¡¨å¤§å°: {config.get('vocab_size')}")
        # é‡è¦: æ£€æŸ¥è¿™ä¸ªå¤§å°æ˜¯å¦åŒ¹é…meta.dvoc

def fix_qwen3_specific_issues(model_dir_path):
    # 1. ä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹åŠ è½½æ–¹å¼
    from transformers import AutoModelForCausalLM, AutoTokenizer
    
    # 2. éªŒè¯æ¨¡å‹é…ç½®æ˜¯å¦æ­£ç¡®
    import json
    with open(os.path.join(model_dir_path, "config.json"), "r") as f:
        config = json.load(f)
    
    # 3. ç¡®è®¤é…ç½®ä¸­çš„å…³é”®å­—æ®µ
    expected_fields = [
        "model_type", "vocab_size", "hidden_size", "num_attention_heads",
        "num_key_value_heads", "intermediate_size"
    ]
    
    for field in expected_fields:
        if field not in config:
            print(f"âš ï¸ é…ç½®æ–‡ä»¶ç¼ºå°‘å…³é”®å­—æ®µ: {field}")
    
    # 4. ä½¿ç”¨ç›¸åŒç‰ˆæœ¬çš„transformersåº“
    import transformers
    print(f"å½“å‰transformersç‰ˆæœ¬: {transformers.__version__}")
    print("æ¨èç‰ˆæœ¬: è¯·å‚è€ƒQwen3å®˜æ–¹æ–‡æ¡£")
def debug_pointers_before_call(self):
    """éªŒè¯æ‰€æœ‰å…³é”®æŒ‡é’ˆåœ¨è°ƒç”¨C++å‰çš„æœ‰æ•ˆæ€§"""
    print("\n==== C++è°ƒç”¨å‰æŒ‡é’ˆéªŒè¯ ====")
    
    # 1. åŸºæœ¬ç»“æ„ä½“
    print(f"metaç»“æ„ä½“åœ°å€: {ctypes.addressof(self.meta):#x}")
    print(f"weights.c_structç»“æ„ä½“åœ°å€: {ctypes.addressof(self.weights.c_struct):#x}")
    
    # 2. åŸºæœ¬å­—æ®µ
    print(f"nlayer: Python={self.meta.nlayer}, Cç»“æ„ä½“={self.weights.c_struct.nlayer}")
    
    # 3. æƒé‡æŒ‡é’ˆ - æ£€æŸ¥ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ª
    print("\nå…³é”®æƒé‡æŒ‡é’ˆéªŒè¯:")
    nlayer = self.meta.nlayer
    
    # è¾“å…¥è¾“å‡ºæƒé‡
    print(f"input_embd: {self.weights.c_struct.input_embd:#x}")
    print(f"output_embd: {self.weights.c_struct.output_embd:#x}")
    print(f"output_norm: {self.weights.c_struct.output_norm:#x}")
    
    # éªŒè¯æ•°ç»„æŒ‡é’ˆ
    print(f"\nå±‚æƒé‡æ•°ç»„æŒ‡é’ˆ:")
    print(f"attn_normæ•°ç»„: {ctypes.addressof(self.weights.c_struct.attn_norm.contents) if self.weights.c_struct.attn_norm else 'NULL'}")
    print(f"attn_q_projæ•°ç»„: {ctypes.addressof(self.weights.c_struct.attn_q_proj.contents) if self.weights.c_struct.attn_q_proj else 'NULL'}")
    
    # éªŒè¯ç¬¬0å±‚å’Œæœ€åä¸€å±‚çš„æŒ‡é’ˆ
    print(f"\nç¬¬ä¸€å±‚(0)æƒé‡æŒ‡é’ˆ:")
    print(f"  attn_norm[0]: {self.weights.c_struct.attn_norm[0]:#x}")
    print(f"  attn_q_proj[0]: {self.weights.c_struct.attn_q_proj[0]:#x}")
    print(f"  attn_k_proj[0]: {self.weights.c_struct.attn_k_proj[0]:#x}")
    print(f"  attn_v_proj[0]: {self.weights.c_struct.attn_v_proj[0]:#x}")
    
    print(f"\næœ€åä¸€å±‚({nlayer-1})æƒé‡æŒ‡é’ˆ:")
    print(f"  attn_norm[{nlayer-1}]: {self.weights.c_struct.attn_norm[nlayer-1]:#x}")
    print(f"  attn_q_proj[{nlayer-1}]: {self.weights.c_struct.attn_q_proj[nlayer-1]:#x}")
def test():
    if len(sys.argv) < 2:
        print("Usage: python qwen3.py <path/to/model_dir> [device] [n_device]")
        sys.exit(1)
        
    model_path = sys.argv[1]  # ä»å‘½ä»¤è¡Œå‚æ•°è·å–æ¨¡å‹è·¯å¾„
    device_type = DeviceType.DEVICE_TYPE_CPU
    
    if len(sys.argv) > 2:
        if sys.argv[2] == "--cpu":
            device_type = DeviceType.DEVICE_TYPE_CPU
        elif sys.argv[2] == "--nvidia":
            device_type = DeviceType.DEVICE_TYPE_NVIDIA

    ndev = int(sys.argv[3]) if len(sys.argv) > 3 else 1
    
    print(f"âœ“ ä½¿ç”¨Qwen3æ¨¡å‹: {model_path}")
    print(f"âœ“ è®¾å¤‡: {device_type}, è®¾å¤‡æ•°: {ndev}")
    
    model = QwenForCausalLM(model_path, device_type, ndev)
    
    # è¯Šæ–­è¯æ±‡è¡¨åŒ¹é…é—®é¢˜
    fix_tokenizer_model_mismatch(model, model_path)

    verify_model_weights(model_path)

    fix_qwen3_specific_issues(model_path)
    
    # è¯Šæ–­C++è®¡ç®—é—®é¢˜
    model.diagnose_cpp_computation()
    
    # ç„¶åæµ‹è¯•ç”Ÿæˆ
    model.generate("å±±ä¸œæœ€é«˜çš„å±±æ˜¯ï¼Ÿ", 5, topp_=0.8, topk_=50, temperature_=0.7)
    model.destroy_model_instance()

if __name__ == "__main__":
    test()