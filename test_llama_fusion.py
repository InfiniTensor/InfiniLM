"""
test_llama_fusion.py - Llama æ¨¡å‹èåˆé›†æˆéªŒè¯è„šæœ¬

æµ‹è¯• LlamaMLP å’Œ LlamaDecoderLayer ä¸­çš„èåˆé€»è¾‘æ˜¯å¦æ­£ç¡®é›†æˆã€‚
"""

import sys

def test_import_fusion_utils():
    """æµ‹è¯• fusion_utils å¯¼å…¥"""
    print("=" * 50)
    print("Test 1: Import fusion_utils")
    print("=" * 50)
    
    try:
        from infinilm.fusion_utils import (
            create_swiglu_pattern,
            create_add_rms_norm_pattern,
            LLMFusionContext,
            FusionManager
        )
        print("âœ… All fusion_utils imports successful!")
        
        # éªŒè¯æ¨¡å¼åˆ›å»º
        swiglu = create_swiglu_pattern()
        add_rms = create_add_rms_norm_pattern()
        print(f"  - SwiGLU pattern: {len(swiglu)} nodes")
        print(f"  - Add+RMSNorm pattern: {len(add_rms)} nodes")
        return True
    except Exception as e:
        print(f"âŒ Import failed: {e}")
        return False

def test_llama_config_fusion_toggle():
    """æµ‹è¯• LlamaConfig çš„ enable_fusion å¼€å…³"""
    print("\n" + "=" * 50)
    print("Test 2: LlamaConfig enable_fusion toggle")
    print("=" * 50)
    
    try:
        from infinilm.models.llama import LlamaConfig
        
        # æµ‹è¯•é»˜è®¤å¼€å¯
        config_on = LlamaConfig(torch_dtype='float16')
        print(f"  Default enable_fusion: {config_on.enable_fusion}")
        assert config_on.enable_fusion == True, "Default should be True"
        
        # æµ‹è¯•æ˜¾å¼å…³é—­
        config_off = LlamaConfig(enable_fusion=False, torch_dtype='float16')
        print(f"  Explicit enable_fusion=False: {config_off.enable_fusion}")
        assert config_off.enable_fusion == False, "Should be False when set"
        
        print("âœ… LlamaConfig enable_fusion toggle works!")
        return True
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        return False

def test_llama_mlp_has_config():
    """æµ‹è¯• LlamaMLP æ˜¯å¦ä¿å­˜äº† config"""
    print("\n" + "=" * 50)
    print("Test 3: LlamaMLP has self.config")
    print("=" * 50)
    
    try:
        from infinilm.models.llama import LlamaConfig
        from infinilm.models.llama.modeling_llama import LlamaMLP
        import infinicore
        
        config = LlamaConfig(
            hidden_size=256,
            intermediate_size=512,
            torch_dtype='float16'
        )
        
        # åˆ›å»º MLP (ä¸éœ€è¦ GPUï¼Œåªæ£€æŸ¥ç»“æ„)
        mlp = LlamaMLP(config)
        
        assert hasattr(mlp, 'config'), "LlamaMLP should have self.config"
        assert mlp.config.enable_fusion == True, "enable_fusion should be accessible"
        
        print(f"  mlp.config exists: {hasattr(mlp, 'config')}")
        print(f"  mlp.config.enable_fusion: {mlp.config.enable_fusion}")
        print("âœ… LlamaMLP correctly stores config!")
        return True
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_llama_decoder_layer_has_config():
    """æµ‹è¯• LlamaDecoderLayer æ˜¯å¦ä¿å­˜äº† config"""
    print("\n" + "=" * 50)
    print("Test 4: LlamaDecoderLayer has self.config")
    print("=" * 50)
    
    try:
        from infinilm.models.llama import LlamaConfig
        from infinilm.models.llama.modeling_llama import LlamaDecoderLayer
        
        config = LlamaConfig(
            hidden_size=256,
            intermediate_size=512,
            num_attention_heads=4,
            num_key_value_heads=4,
            torch_dtype='float16'
        )
        
        layer = LlamaDecoderLayer(config, layer_idx=0)
        
        assert hasattr(layer, 'config'), "LlamaDecoderLayer should have self.config"
        assert layer.config.enable_fusion == True, "enable_fusion should be accessible"
        
        print(f"  layer.config exists: {hasattr(layer, 'config')}")
        print(f"  layer.config.enable_fusion: {layer.config.enable_fusion}")
        print("âœ… LlamaDecoderLayer correctly stores config!")
        return True
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    print("\n" + "#" * 60)
    print("  Llama Fusion Integration Test Suite")
    print("#" * 60 + "\n")
    
    results = []
    
    results.append(("Import fusion_utils", test_import_fusion_utils()))
    results.append(("LlamaConfig toggle", test_llama_config_fusion_toggle()))
    results.append(("LlamaMLP has config", test_llama_mlp_has_config()))
    results.append(("LlamaDecoderLayer has config", test_llama_decoder_layer_has_config()))
    
    # æ±‡æ€»
    print("\n" + "=" * 60)
    print("Summary")
    print("=" * 60)
    
    passed = sum(1 for _, r in results if r)
    total = len(results)
    
    for name, result in results:
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"  {status}: {name}")
    
    print(f"\nTotal: {passed}/{total} tests passed")
    
    if passed == total:
        print("\nğŸ‰ All Phase 5 integration tests passed!")
        return 0
    else:
        print("\nâš ï¸  Some tests failed. Please check the output above.")
        return 1

if __name__ == "__main__":
    sys.exit(main())
