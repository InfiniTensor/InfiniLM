#!/usr/bin/env python3
"""
Fixed Qwen3 Implementation - Using New qw C++ API
This version properly implements Qwen3 support using the dedicated qwen3 C++ API
with proper Q/K normalization and parameter mapping.

Key Improvements:
1. Uses dedicated Qwen3 API instead of fallback jiuge API
2. Handles Q/K normalization weights properly
3. Implements separate QKV projections
4. One-to-one parameter mapping following jiuge.py patterns
"""

from typing import List, Optional
import os
import sys
import time
import json
import torch
import transformers
from pathlib import Path
from ctypes import POINTER, c_float, c_int, c_uint, c_void_p, byref
import safetensors
import ctypes

# Set default device
torch.set_default_device("cpu")

# Import the proper Qwen3 API
try:
    from libinfinicore_infer import (
        Qwen3MetaCStruct,
        Qwen3WeightsCStruct,
        create_qwen3_model,
        destroy_qwen3_model,
        create_qwen3_kv_cache,
        drop_qwen3_kv_cache,
        infer_qwen3_batch,
        DataType,
        DeviceType,
        KVCacheCStruct,
    )
    QWEN3_API_AVAILABLE = True
    print("âœ“ Qwen3 C++ API available")
except ImportError as e:
    print(f"âš  Qwen3 C++ API not available: {e}")
    print("  This version requires the qw implementation")
    sys.exit(1)

from infer_task import Qwen3InferTask, Qwen3KVCache


class Qwen3WeightsNaming:
    """Qwen3-specific weight naming with Q/K normalization and separate QKV support"""
    
    def input_embd(self):
        return "model.embed_tokens.weight"

    def output_norm(self):
        return "model.norm.weight"

    def output_embd(self):
        return "lm_head.weight"

    def attn_norm(self, i):
        return f"model.layers.{i}.input_layernorm.weight"

    def attn_q(self, i):
        return f"model.layers.{i}.self_attn.q_proj.weight"

    def attn_k(self, i):
        return f"model.layers.{i}.self_attn.k_proj.weight"

    def attn_v(self, i):
        return f"model.layers.{i}.self_attn.v_proj.weight"

    def attn_o(self, i):
        return f"model.layers.{i}.self_attn.o_proj.weight"

    def ffn_norm(self, i):
        return f"model.layers.{i}.post_attention_layernorm.weight"

    def gate(self, i):
        return f"model.layers.{i}.mlp.gate_proj.weight"

    def up(self, i):
        return f"model.layers.{i}.mlp.up_proj.weight"

    def down(self, i):
        return f"model.layers.{i}.mlp.down_proj.weight"

    # Qwen3-specific Q/K normalization weights
    def q_norm(self, i):
        return f"model.layers.{i}.self_attn.q_norm.weight"

    def k_norm(self, i):
        return f"model.layers.{i}.self_attn.k_norm.weight"

    @staticmethod
    def match(state_dict):
        """Check if state_dict matches Qwen3 naming pattern"""
        has_basic = (
            "model.norm.weight" in state_dict
            and "model.layers.0.self_attn.q_proj.weight" in state_dict
        )
        # Qwen3 often has q_norm and k_norm weights
        has_qk_norm = (
            "model.layers.0.self_attn.q_norm.weight" in state_dict
            and "model.layers.0.self_attn.k_norm.weight" in state_dict
        )
        return has_basic and has_qk_norm

class Qwen3MetaFromConfig(Qwen3MetaCStruct):
    """Qwen3 metadata structure from model config"""
    
    def __init__(self, config, dtype=torch.float16, max_tokens=None):
        super().__init__()  # å…ˆè°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°
        
        if dtype == torch.float16:
            dt_ = DataType.INFINI_DTYPE_F16
        elif dtype == torch.float32:
            dt_ = DataType.INFINI_DTYPE_F32
        elif dtype == torch.bfloat16:
            dt_ = DataType.INFINI_DTYPE_BF16
        else:
            dt_ = DataType.INFINI_DTYPE_F16

        # è®¾ç½®å­—æ®µå€¼
        self.dt_logits = dt_
        self.nlayer = config["num_hidden_layers"]
        self.d = config["hidden_size"]
        self.nh = config["num_attention_heads"]
        self.nkvh = (
            config["num_key_value_heads"]
            if "num_key_value_heads" in config
            else config["num_attention_heads"]
        )
        self.dh = config["hidden_size"] // config["num_attention_heads"]
        self.di = config["intermediate_size"]
        self.dctx = (
            config["max_position_embeddings"] if max_tokens is None else max_tokens
        )
        self.dvoc = config["vocab_size"]
        self.epsilon = config.get("rms_norm_eps", 1e-6)
        self.theta = config.get("rope_theta", 10000.0)
        self.bos_token = config.get("bos_token_id", 1)
        self.end_token = config.get("eos_token_id", 2)
        self.attn_dropout = config.get("attention_dropout", 0.0)
        self.tie_embd = config.get("tie_word_embeddings", True)
        
        self.torch_dtype_logits = dtype

class Qwen3WeightsImpl(Qwen3WeightsCStruct):
    """Qwen3 weights implementation with Q/K normalization support"""
    
    def __init__(
        self,
        meta,
        naming,
        state_dict,
        torch_dt_mat=torch.float16,
        torch_dt_norm=torch.float32,
        ndev=1,
        transpose_weight=True,
    ):
        nlayer = meta.nlayer
        nh = meta.nh
        nkvh = meta.nkvh
        dh = meta.dh
        d = meta.d
        di = meta.di
        
        assert nh % nkvh == 0
        assert nh % ndev == 0
        assert nkvh % ndev == 0
        assert di % ndev == 0
        
        torch_dt_logits = meta.torch_dtype_logits
        
        # è°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°
        super().__init__()
        
        # Set data types
        if torch_dt_mat == torch.float16:
            self.dt_mat = DataType.INFINI_DTYPE_F16
        elif torch_dt_mat == torch.float32:
            self.dt_mat = DataType.INFINI_DTYPE_F32
        elif torch_dt_mat == torch.bfloat16:
            self.dt_mat = DataType.INFINI_DTYPE_BF16
        else:
            raise ValueError("Unsupported proj weight data type")
            
        if torch_dt_norm == torch.float16:
            self.dt_norm = DataType.INFINI_DTYPE_F16
        elif torch_dt_norm == torch.float32:
            self.dt_norm = DataType.INFINI_DTYPE_F32
        elif torch_dt_norm == torch.bfloat16:
            self.dt_norm = DataType.INFINI_DTYPE_BF16
        else:
            raise ValueError("Unsupported norm weight data type")

        # è®¾ç½®ç»“æ„ä½“å­—æ®µ
        self.nlayer = nlayer
        self.transpose_linear_weights = 1 if transpose_weight else 0

        # Determine input/output embedding names
        input_embd_naming = (
            naming.input_embd()
            if naming.input_embd() in state_dict
            else naming.output_embd()
        )
        output_embd_naming = (
            naming.output_embd()
            if naming.output_embd() in state_dict
            else naming.input_embd()
        )
        
        # Basic weights
        self.input_embd_tensor = state_dict[input_embd_naming].to(torch_dt_logits)
        self.input_embd = self.input_embd_tensor.data_ptr()
        
        self.output_norm_tensor = state_dict[naming.output_norm()].to(torch_dt_norm)
        self.output_norm = self.output_norm_tensor.data_ptr()
        
        self.output_embd_tensor = state_dict[output_embd_naming].to(torch_dt_mat)
        if not transpose_weight:
            self.output_embd_tensor = self.output_embd_tensor.transpose(0, 1).contiguous()
        self.output_embd = self.output_embd_tensor.data_ptr()

        # Attention layer normalization weights
        self.attn_norm_tensors = [
            state_dict[naming.attn_norm(i)].to(torch_dt_norm) for i in range(nlayer)
        ]
        self.attn_norm_ptrs = [
            self.attn_norm_tensors[i].data_ptr() for i in range(nlayer)
        ]
        self.attn_norm = (c_void_p * nlayer)(*self.attn_norm_ptrs)

        # Q/K normalization weights
        self.attn_q_norm_tensors = []
        self.attn_k_norm_tensors = []
        if hasattr(naming, 'q_norm'):
            try:
                for i in range(nlayer):
                    self.attn_q_norm_tensors.append(state_dict[naming.q_norm(i)].to(torch_dt_norm))
                    self.attn_k_norm_tensors.append(state_dict[naming.k_norm(i)].to(torch_dt_norm))
                
                self.attn_q_norm_ptrs = [self.attn_q_norm_tensors[i].data_ptr() for i in range(nlayer)]
                self.attn_k_norm_ptrs = [self.attn_k_norm_tensors[i].data_ptr() for i in range(nlayer)]
                self.attn_q_norm = (c_void_p * nlayer)(*self.attn_q_norm_ptrs)
                self.attn_k_norm = (c_void_p * nlayer)(*self.attn_k_norm_ptrs)
                
                print(f"âœ“ Loaded Q/K normalization weights for {nlayer} layers")
            except KeyError as e:
                print(f"âš  Q/K norm weights not found: {e}")
                # åˆ›å»ºç©ºæŒ‡é’ˆæ•°ç»„
                null_ptrs = [None for _ in range(nlayer)]
                self.attn_q_norm = (c_void_p * nlayer)(*null_ptrs)
                self.attn_k_norm = (c_void_p * nlayer)(*null_ptrs)
        else:
            # åˆ›å»ºç©ºæŒ‡é’ˆæ•°ç»„
            null_ptrs = [None for _ in range(nlayer)]
            self.attn_q_norm = (c_void_p * nlayer)(*null_ptrs)
            self.attn_k_norm = (c_void_p * nlayer)(*null_ptrs)

        # åˆ†ç¦»çš„ Q, K, V æŠ•å½±æƒé‡
        self.attn_q_proj_tensors = []
        self.attn_k_proj_tensors = []
        self.attn_v_proj_tensors = []
        
        for i in range(nlayer):
            q_tensor = state_dict[naming.attn_q(i)].to(torch_dt_mat)
            k_tensor = state_dict[naming.attn_k(i)].to(torch_dt_mat)
            v_tensor = state_dict[naming.attn_v(i)].to(torch_dt_mat)
            
            if not transpose_weight:
                q_tensor = q_tensor.transpose(0, 1).contiguous()
                k_tensor = k_tensor.transpose(0, 1).contiguous()
                v_tensor = v_tensor.transpose(0, 1).contiguous()
            
            self.attn_q_proj_tensors.append(q_tensor)
            self.attn_k_proj_tensors.append(k_tensor)
            self.attn_v_proj_tensors.append(v_tensor)

        self.attn_q_proj_ptrs = [self.attn_q_proj_tensors[i].data_ptr() for i in range(nlayer)]
        self.attn_k_proj_ptrs = [self.attn_k_proj_tensors[i].data_ptr() for i in range(nlayer)]
        self.attn_v_proj_ptrs = [self.attn_v_proj_tensors[i].data_ptr() for i in range(nlayer)]
        
        self.attn_q_proj = (c_void_p * nlayer)(*self.attn_q_proj_ptrs)
        self.attn_k_proj = (c_void_p * nlayer)(*self.attn_k_proj_ptrs)
        self.attn_v_proj = (c_void_p * nlayer)(*self.attn_v_proj_ptrs)

        # Attention output weights
        self.attn_o_proj_tensors = []
        for i in range(nlayer):
            o_tensor = state_dict[naming.attn_o(i)].to(torch_dt_mat)
            if not transpose_weight:
                o_tensor = o_tensor.transpose(0, 1).contiguous()
            self.attn_o_proj_tensors.append(o_tensor)
            
        self.attn_o_proj_ptrs = [self.attn_o_proj_tensors[i].data_ptr() for i in range(nlayer)]
        self.attn_o_proj = (c_void_p * nlayer)(*self.attn_o_proj_ptrs)

        # FFN weights
        self.mlp_norm_tensors = [
            state_dict[naming.ffn_norm(i)].to(torch_dt_norm) for i in range(nlayer)
        ]
        self.mlp_norm_ptrs = [self.mlp_norm_tensors[i].data_ptr() for i in range(nlayer)]
        self.mlp_norm = (c_void_p * nlayer)(*self.mlp_norm_ptrs)

        # åˆ†ç¦»çš„ gate å’Œ up æŠ•å½±
        self.mlp_gate_proj_tensors = []
        self.mlp_up_proj_tensors = []
        self.mlp_down_proj_tensors = []
        
        for i in range(nlayer):
            gate_tensor = state_dict[naming.gate(i)].to(torch_dt_mat)
            up_tensor = state_dict[naming.up(i)].to(torch_dt_mat)
            down_tensor = state_dict[naming.down(i)].to(torch_dt_mat)
            
            if not transpose_weight:
                gate_tensor = gate_tensor.transpose(0, 1).contiguous()
                up_tensor = up_tensor.transpose(0, 1).contiguous()
                down_tensor = down_tensor.transpose(0, 1).contiguous()
            
            self.mlp_gate_proj_tensors.append(gate_tensor)
            self.mlp_up_proj_tensors.append(up_tensor)
            self.mlp_down_proj_tensors.append(down_tensor)

        self.mlp_gate_proj_ptrs = [self.mlp_gate_proj_tensors[i].data_ptr() for i in range(nlayer)]
        self.mlp_up_proj_ptrs = [self.mlp_up_proj_tensors[i].data_ptr() for i in range(nlayer)]
        self.mlp_down_proj_ptrs = [self.mlp_down_proj_tensors[i].data_ptr() for i in range(nlayer)]
        
        self.mlp_gate_proj = (c_void_p * nlayer)(*self.mlp_gate_proj_ptrs)
        self.mlp_up_proj = (c_void_p * nlayer)(*self.mlp_up_proj_ptrs)
        self.mlp_down_proj = (c_void_p * nlayer)(*self.mlp_down_proj_ptrs)

        # éªŒè¯æ‰€æœ‰å…³é”®æƒé‡éƒ½å·²åŠ è½½
        required_weights = [
            self.input_embd_tensor,
            self.output_embd_tensor,
            self.output_norm_tensor,
        ]
        
        for i, tensor in enumerate(required_weights):
            if tensor is None or tensor.data_ptr() == 0:
                raise RuntimeError(f"Critical weight {i} is None or has null data pointer")
        
        # éªŒè¯å±‚æƒé‡
        for i in range(nlayer):
            critical_tensors = [
                self.attn_norm_tensors[i],
                self.attn_q_proj_tensors[i],
                self.attn_k_proj_tensors[i],
                self.attn_v_proj_tensors[i],
                self.attn_o_proj_tensors[i],
                self.mlp_norm_tensors[i],
                self.mlp_gate_proj_tensors[i],
                self.mlp_up_proj_tensors[i],
                self.mlp_down_proj_tensors[i],
            ]
            
            for j, tensor in enumerate(critical_tensors):
                if tensor is None or tensor.data_ptr() == 0:
                    raise RuntimeError(f"Layer {i} weight {j} is None or has null data pointer")
        
        print(f"âœ“ All {nlayer} layers' weights validated successfully")
        print("ğŸ” First 10 values of attn_q_0:")
        print(self.attn_q_proj_tensors[0].flatten()[:10])
        print("ğŸ” Pointer address:", hex(self.attn_q_proj_tensors[0].data_ptr()))

class Qwen3BatchedTask:
    """Batched inference task for Qwen3"""
    
    def __init__(self, tasks: List[Qwen3InferTask]):
        self.tasks = tasks
        self.nreq = len(tasks)

        # Precompute fields
        token_lists = [t.tokens for t in tasks]
        self.req_lens_list = [len(toks) for toks in token_lists]
        self.req_pos_list = [t.pos for t in tasks]
        self.kv_cache_ptrs = [t.kvcache().data() for t in tasks]
        self.temperatures_list = [t.temperature for t in tasks]
        self.topks_list = [t.topk for t in tasks]
        self.topps_list = [t.topp for t in tasks]

        # Flatten token lists
        flat_tokens = [tok for toks in token_lists for tok in toks]
        self.ntok = len(flat_tokens)

        # Convert to ctypes arrays
        self.tokens = (c_uint * self.ntok)(*flat_tokens)
        self.req_lens = (c_uint * self.nreq)(*self.req_lens_list)
        self.req_pos = (c_uint * self.nreq)(*self.req_pos_list)
        self.kv_caches = (POINTER(KVCacheCStruct) * self.nreq)(*self.kv_cache_ptrs)
        self.temperatures = (c_float * self.nreq)(*self.temperatures_list)
        self.topks = (c_uint * self.nreq)(*self.topks_list)
        self.topps = (c_float * self.nreq)(*self.topps_list)

    def input_args(self):
        return (
            self.tokens,
            self.ntok,
            self.req_lens,
            self.nreq,
            self.req_pos,
            self.kv_caches,
            self.temperatures,
            self.topks,
            self.topps,
        )


class QwenForCausalLM:
    """Qwen3 model for causal language modeling - FIXED VERSION"""
    
    def __init__(
        self, model_dir_path, device=DeviceType.DEVICE_TYPE_CPU, ndev=1, max_tokens=None
    ):
        def load_all_safetensors_from_dir(dir_path_: str):
            tensors_ = {}
            dir_path_ = Path(dir_path_)
            for file in sorted(dir_path_.glob("*.safetensors")):
                data_ = safetensors.safe_open(file, "pt")
                for name_ in data_.keys():
                    tensors_[name_] = data_.get_tensor(name_)
            return tensors_

        print("Loading Qwen3 model weights to host...")
        load_start_time = time.time()

        with open(os.path.join(model_dir_path, "config.json"), "r") as f:
            config = json.load(f)
            self.config = config
            
        eos_token_id = self.config.get("eos_token_id", 2)
        self.eos_token_id = (
            [eos_token_id] if type(eos_token_id) == int else eos_token_id
        )
        
        transpose_weight = (
            device != DeviceType.DEVICE_TYPE_ASCEND
        )

        # Load state dict
        if any(file.suffix == ".safetensors" for file in Path(model_dir_path).iterdir()):
            state_dict = load_all_safetensors_from_dir(model_dir_path)
        else:
            state_dict = torch.load(
                os.path.join(model_dir_path, "pytorch_model.bin"),
                weights_only=True,
                map_location="cpu",
            )

        # Determine naming scheme
        if Qwen3WeightsNaming.match(state_dict):
            print("âœ“ Using Qwen3WeightsNaming (with Q/K normalization)")
                    # Create metadata and weights
            self.meta = Qwen3MetaFromConfig(config, max_tokens=max_tokens)
            self.weights = Qwen3WeightsImpl(
                self.meta,
                Qwen3WeightsNaming(),
                state_dict,
                ndev=ndev,
                transpose_weight=transpose_weight,
            )
            # Load tokenizer
            self.tokenizer = transformers.AutoTokenizer.from_pretrained(
                model_dir_path, trust_remote_code=True
            )
        elif LlamaWeightsNaming.match(state_dict):
            print("âš  Using LlamaWeightsNaming (fallback, no Q/K normalization)")
        else:
            raise ValueError("Unsupported weight naming scheme")




        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        load_end_time = time.time()
        print(f"Weight loading time: {load_end_time - load_start_time:.3f}s")

        print(f"Creating Qwen3 model on {ndev} devices...")
        load_start_time = time.time()
        dev_ids = (c_int * ndev)(*[i for i in range(ndev)])
    
        try:
            self.model_instance = create_qwen3_model(
                ctypes.byref(self.meta), 
                ctypes.byref(self.weights),
                device, 
                ndev,
                dev_ids
            )
            print(f"âœ“ Model created successfully")
        except Exception as e:
            print(f"âœ— Error creating model: {e}")
            import traceback
            traceback.print_exc()
            raise

        load_end_time = time.time()
        print(f"Model creation time: {load_end_time - load_start_time:.3f}s")
        if self.model_instance is None:
            raise RuntimeError("Model instance is None after creation")

    def max_context_len(self):
        return self.meta.dctx

    def create_kv_cache(self):
        # FIXED: Use proper Qwen3 KV cache API
        return create_qwen3_kv_cache(self.model_instance)

    def drop_kv_cache(self, kv_cache):
        # FIXED: Use proper Qwen3 KV cache API
        drop_qwen3_kv_cache(self.model_instance, kv_cache)

    def batch_infer_one_round(self, tasks: List[Qwen3InferTask]):
        output = (c_uint * len(tasks))()
        
        # ä½¿ç”¨Qwen3BatchedTaskç±»æ¥å¤„ç†æ‰¹å¤„ç†
        batch_inputs = Qwen3BatchedTask(tasks)


        # è¯¦ç»†éªŒè¯è¾“å…¥å‚æ•°
        # print(f"DEBUG Batch inference:")
        # print(f"  Number of requests: {batch_inputs.nreq}")
        # print(f"  Total tokens: {batch_inputs.ntok}")
        # print(f"  Request lengths: {batch_inputs.req_lens_list}")
        # print(f"  Request positions: {batch_inputs.req_pos_list}")
        # print(f"  Temperatures: {batch_inputs.temperatures_list}")
        # print(f"  Top-k values: {batch_inputs.topks_list}")
        # print(f"  Top-p values: {batch_inputs.topps_list}")
        
        # æ£€æŸ¥C++å‡½æ•°è°ƒç”¨å‚æ•°çš„å†…å­˜åœ°å€ - FIXED
        print(f"ğŸ” C++ function parameters:")
        try:
            model_ptr_addr = ctypes.addressof(self.model_instance.contents) if self.model_instance else 0
            print(f"  model_instance: {hex(model_ptr_addr)}")
        except:
            print(f"  model_instance: exists={self.model_instance is not None}")
            
        print(f"  tokens array ptr: {hex(ctypes.addressof(batch_inputs.tokens))}")
        print(f"  kv_caches array ptr: {hex(ctypes.addressof(batch_inputs.kv_caches))}")
        print(f"  output array ptr: {hex(ctypes.addressof(output))}")

            # æ£€æŸ¥è¾“å…¥tokençš„åˆç†æ€§
        if batch_inputs.ntok > 0:
            first_few_tokens = list(batch_inputs.tokens)[:min(5, batch_inputs.ntok)]
            print(f"  First few input tokens: {first_few_tokens}")
        
        
        # éªŒè¯è¾“å…¥å‚æ•°
        if batch_inputs.ntok == 0:
            raise ValueError("æ²¡æœ‰tokenséœ€è¦å¤„ç†")
        if batch_inputs.nreq == 0:
            raise ValueError("æ²¡æœ‰è¯·æ±‚éœ€è¦å¤„ç†")
        
        try:
            # ä½¿ç”¨batch_inputsä¸­çš„æ•°ç»„
            print("ğŸš€ Calling infer_qwen3_batch...")
            infer_qwen3_batch(
                self.model_instance,
                *batch_inputs.input_args(),
                output,
            )
            print("âœ… infer_qwen3_batch completed")
            
            # éªŒè¯è¾“å‡ºtoken
            for i, token in enumerate(list(output)):
                print(f"  Output token[{i}]: {token}")
                if token >= self.meta.dvoc:
                    print(f"    âš  Invalid: exceeds vocab_size {self.meta.dvoc}")
                if token < 0:
                    print(f"    âš  Invalid: negative token")
                    
        except Exception as e:
            print(f"âŒ C++ inference failed: {e}")
            import traceback
            traceback.print_exc()
            raise
            
        return list(output)
    def generate(self, input_content, max_steps, topp_=0.8, topk_=50, temperature_=0.7):
        # Apply chat template if available
        if hasattr(self.tokenizer, 'chat_template') and self.tokenizer.chat_template:
            input_content = self.tokenizer.apply_chat_template(
                conversation=[{"role": "user", "content": input_content}],
                add_generation_prompt=True,
                tokenize=False,
            )
        
        print(input_content, end="", flush=True)
        tokens = self.tokenizer.encode(input_content)

        # æ·»åŠ è¯¦ç»†è°ƒè¯•ä¿¡æ¯
        print(f"\nDEBUG: Input tokens: {tokens}")
        print(f"DEBUG: Token count: {len(tokens)}")
        print(f"DEBUG: EOS tokens: {self.eos_token_id}")
        print(f"DEBUG: Vocab size: {self.meta.dvoc}")

            # éªŒè¯è¾“å…¥tokençš„åˆç†æ€§
        print("ğŸ” Validating input tokens:")
        for i, token in enumerate(tokens[:5]):  # åªæ£€æŸ¥å‰5ä¸ª
            if token >= self.meta.dvoc or token < 0:
                raise ValueError(f"âŒ Invalid input token[{i}]: {token} (vocab_size: {self.meta.dvoc})")
            
            # æ£€æŸ¥tokenå¯¹åº”çš„embedding
            embd_vec = self.weights.input_embd_tensor[token]
            embd_norm = embd_vec.norm().item()
            print(f"    Token[{i}]={token}, embedding_norm={embd_norm:.6f}")
            
            if embd_norm < 1e-8:
                print(f"    âš  Warning: Token {token} has very small embedding norm")
            if torch.isnan(embd_vec).any() or torch.isinf(embd_vec).any():
                raise RuntimeError(f"âŒ Token {token} embedding contains NaN/Inf")
            
        infer_task = Qwen3InferTask(
            tokens=tokens,
            position=0,
            temperature=temperature_,
            topk=topk_,
            topp=topp_,
            end_tokens=self.eos_token_id,
            max_tokens=int(self.meta.dctx),
            task_id=0
        )

        infer_task.bind_kvcache(Qwen3KVCache(self))
        # éªŒè¯æ¨¡å‹å®ä¾‹çŠ¶æ€ - FIXED
        print(f"ğŸ” Model instance validation:")
        try:
            model_ptr_addr = ctypes.addressof(self.model_instance.contents) if self.model_instance else 0
            print(f"    Model instance ptr: {hex(model_ptr_addr)}")
        except:
            # é™çº§å¤„ç†
            print(f"    Model instance: {self.model_instance is not None}")
            
        if self.model_instance is None:
            raise RuntimeError("âŒ Model instance is null before inference")


        steps = 0
        total_time = 0
        output_content = ""

        for step_i in range(max_steps):
            start_time = time.time()
            output_tokens = self.batch_infer_one_round([infer_task])
            end_time = time.time()
            steps += 1
                    # è¯¦ç»†çš„tokenåˆ†æ
            output_token = output_tokens[0]
            print(f"\nDEBUG Step {step_i}:")
            print(f"  Output token ID: {output_token}")
            print(f"  Token in vocab range: {0 <= output_token < self.meta.dvoc}")
            print(f"  Is EOS token: {output_token in self.eos_token_id}")
            # æ£€æŸ¥tokenåˆç†æ€§
            if output_token >= self.meta.dvoc:
                print(f"  âš  WARNING: Token {output_token} exceeds vocab size {self.meta.dvoc}")
                break
            if output_token < 0:
                print(f"  âš  WARNING: Negative token ID {output_token}")
                break
            
            try:
                output_str = self.tokenizer.decode([output_token], skip_special_tokens=False)
                print(f"  Decoded: '{output_str}'")
            except Exception as e:
                print(f"  âš  Decode failed: {e}")
                output_str = self.tokenizer._tokenizer.id_to_token(output_token)
                if output_str is None:
                    output_str = f"[UNK_{output_token}]"
                else:
                    output_str = output_str.replace("â–", " ").replace("<0x0A>", "\n")
            
            output_content += output_str
            print(output_str, end="", flush=True)
            
            if output_tokens[0] in self.eos_token_id:
                break
                
            infer_task.next(output_tokens[0])

            if step_i > 0:
                total_time += end_time - start_time

        print("\n")
        avg_time = total_time * 1000 / (steps - 1) if steps > 1 else 0
        print(f"Time per step: {avg_time:.3f}ms")

        try:
            infer_task._kv_cache.drop()
        except AttributeError:
            # å¦‚æœdropæ–¹æ³•æœ‰é—®é¢˜ï¼Œè·³è¿‡æ¸…ç†
            print("    âš  KV cache cleanup skipped (method issue)")
        except Exception as e:
            print(f"    âš  KV cache cleanup failed: {e}")

        return output_content, avg_time

    def destroy_model_instance(self):
        # FIXED: Use proper Qwen3 model destruction API
        destroy_qwen3_model(self.model_instance)
        print("Qwen3 Model destroyed")

    def diagnose_cpp_computation(self):
        """è¯Šæ–­C++æ¨ç†å¼•æ“çš„è®¡ç®—æ­£ç¡®æ€§"""
        
        print(f"\n{'='*60}")
        print("ğŸ”¬ C++ COMPUTATION DIAGNOSIS")
        print(f"{'='*60}")
        
        # 1. æµ‹è¯•å›ºå®šè¾“å…¥çš„ä¸€è‡´æ€§
        print("\n1ï¸âƒ£ Testing computation consistency with fixed inputs:")
        
        # ä½¿ç”¨éå¸¸ç®€å•çš„è¾“å…¥
        simple_tokens = [1, 2, 3]  # BOS, simple tokens
        
        results = []
        for i in range(3):  # è¿è¡Œ3æ¬¡ç›¸åŒçš„æ¨ç†
            print(f"\n  Run {i+1}/3:")
            
            task = Qwen3InferTask(
                tokens=simple_tokens,
                position=0,
                temperature=0.0,  # å®Œå…¨ç¡®å®šæ€§
                topk=1,           # åªå–æ¦‚ç‡æœ€é«˜çš„token
                topp=1.0,
                end_tokens=self.eos_token_id,
                max_tokens=int(self.meta.dctx),
                task_id=0
            )
            task.bind_kvcache(Qwen3KVCache(self))
            
            # æ‰§è¡Œæ¨ç†
            output_tokens = self.batch_infer_one_round([task])
            output_token = output_tokens[0]
            
            print(f"    Input tokens: {simple_tokens}")
            print(f"    Output token: {output_token}")
            
            results.append(output_token)
                    # FIXED: ä½¿ç”¨try-exceptæ¥å¤„ç†KVç¼“å­˜æ¸…ç†
            try:
                task._kv_cache.drop()
            except AttributeError:
                # å¦‚æœdropæ–¹æ³•æœ‰é—®é¢˜ï¼Œè·³è¿‡æ¸…ç†
                print("    âš  KV cache cleanup skipped (method issue)")
            except Exception as e:
                print(f"    âš  KV cache cleanup failed: {e}")
        
        # æ£€æŸ¥ä¸€è‡´æ€§
        if len(set(results)) == 1:
            print(f"  âœ… PASS: All runs produced same result: {results[0]}")
        else:
            print(f"  âŒ FAIL: Inconsistent results: {results}")
            print("    This indicates non-deterministic computation or memory corruption")
        
        # 2. æµ‹è¯•ä¸åŒtemperatureçš„å½±å“
        print("\n2ï¸âƒ£ Testing temperature parameter effect:")
        
        temps = [0.0, 0.5, 1.0]
        temp_results = {}
        
        for temp in temps:
            task = Qwen3InferTask(
                tokens=simple_tokens,
                position=0,
                temperature=temp,
                topk=50,
                topp=0.8,
                end_tokens=self.eos_token_id,
                max_tokens=int(self.meta.dctx),
                task_id=0
            )
            task.bind_kvcache(Qwen3KVCache(self))
            
            # è¿è¡Œå¤šæ¬¡è·å–åˆ†å¸ƒ
            temp_outputs = []
            for _ in range(5):
                output_tokens = self.batch_infer_one_round([task])
                temp_outputs.append(output_tokens[0])
                task.next(output_tokens[0])  # æ›´æ–°çŠ¶æ€ä»¥ä¾¿ä¸‹æ¬¡æ¨ç†
            
            temp_results[temp] = temp_outputs
            try:
                task._kv_cache.drop()
            except AttributeError:
                # å¦‚æœdropæ–¹æ³•æœ‰é—®é¢˜ï¼Œè·³è¿‡æ¸…ç†
                print("    âš  KV cache cleanup skipped (method issue)")
            except Exception as e:
                print(f"    âš  KV cache cleanup failed: {e}")

            unique_outputs = len(set(temp_outputs))
            print(f"    temp={temp}: outputs={temp_outputs}, unique={unique_outputs}")
        
        # éªŒè¯temperature=0.0åº”è¯¥å®Œå…¨ç¡®å®š
        if len(set(temp_results[0.0])) == 1:
            print("  âœ… PASS: Temperature=0.0 produces deterministic output")
        else:
            print("  âŒ FAIL: Temperature=0.0 should be deterministic")
        
        # 3. æµ‹è¯•è¾“å…¥é•¿åº¦å¯¹è¾“å‡ºçš„å½±å“
        print("\n3ï¸âƒ£ Testing input length effect:")
        
        test_inputs = [
            [1],           # 1 token
            [1, 2],        # 2 tokens  
            [1, 2, 3],     # 3 tokens
            [1, 2, 3, 4],  # 4 tokens
        ]
        
        length_results = {}
        for tokens in test_inputs:
            task = Qwen3InferTask(
                tokens=tokens,
                position=0,
                temperature=0.0,
                topk=1,
                topp=1.0,
                end_tokens=self.eos_token_id,
                max_tokens=int(self.meta.dctx),
                task_id=0
            )
            task.bind_kvcache(Qwen3KVCache(self))
            
            output_tokens = self.batch_infer_one_round([task])
            length_results[len(tokens)] = output_tokens[0]
            
            print(f"    Input length {len(tokens)}: {tokens} -> {output_tokens[0]}")
            try:
                task._kv_cache.drop()
            except AttributeError:
                # å¦‚æœdropæ–¹æ³•æœ‰é—®é¢˜ï¼Œè·³è¿‡æ¸…ç†
                print("    âš  KV cache cleanup skipped (method issue)")
            except Exception as e:
                print(f"    âš  KV cache cleanup failed: {e}")

        # 4. æµ‹è¯•KVç¼“å­˜çŠ¶æ€çš„å½±å“
        print("\n4ï¸âƒ£ Testing KV cache state impact:")
        
        # ç¬¬ä¸€æ¬¡æ¨ç†
        task1 = Qwen3InferTask(
            tokens=[1, 2, 3],
            position=0,
            temperature=0.0,
            topk=1,
            topp=1.0,
            end_tokens=self.eos_token_id,
            max_tokens=int(self.meta.dctx),
            task_id=0
        )
        task1.bind_kvcache(Qwen3KVCache(self))
        
        output1 = self.batch_infer_one_round([task1])[0]
        print(f"    Fresh KV cache: [1,2,3] -> {output1}")
        
        # ç»§ç»­ç”¨ç›¸åŒçš„KVç¼“å­˜æ¨ç†ä¸‹ä¸€ä¸ªtoken
        task1.next(output1)
        output2 = self.batch_infer_one_round([task1])[0]
        print(f"    Continued KV cache: append {output1} -> {output2}")
        
        # é‡æ–°å¼€å§‹ï¼Œä½†ç”¨ä¸åŒçš„æ–¹å¼
        task2 = Qwen3InferTask(
            tokens=[1, 2, 3, output1],
            position=0,
            temperature=0.0,
            topk=1,
            topp=1.0,
            end_tokens=self.eos_token_id,
            max_tokens=int(self.meta.dctx),
            task_id=0
        )
        task2.bind_kvcache(Qwen3KVCache(self))
        
        output3 = self.batch_infer_one_round([task2])[0]
        print(f"    Fresh KV cache: [1,2,3,{output1}] -> {output3}")
        
        if output2 == output3:
            print("  âœ… PASS: KV cache state consistency maintained")
        else:
            print("  âŒ FAIL: KV cache state inconsistent")
            print(f"         Continued: {output2}, Fresh: {output3}")
        
        task1._kv_cache.drop()
        task2._kv_cache.drop()
        
        # 5. æµ‹è¯•è¾¹ç•Œæƒ…å†µ
        print("\n5ï¸âƒ£ Testing edge cases:")
        
        # æµ‹è¯•vocabè¾¹ç•Œé™„è¿‘çš„token
        edge_tokens = [0, 1, self.meta.dvoc-2, self.meta.dvoc-1]  # é¿å…æ— æ•ˆtoken
        
        for token in edge_tokens:
            if 0 <= token < self.meta.dvoc:
                task = Qwen3InferTask(
                    tokens=[token],
                    position=0,
                    temperature=0.0,
                    topk=1,
                    topp=1.0,
                    end_tokens=self.eos_token_id,
                    max_tokens=int(self.meta.dctx),
                    task_id=0
                )
                task.bind_kvcache(Qwen3KVCache(self))
                
                try:
                    output = self.batch_infer_one_round([task])[0]
                    print(f"    Edge token {token} -> {output}")
                    
                    if 0 <= output < self.meta.dvoc:
                        print(f"      âœ… Output {output} in valid range")
                    else:
                        print(f"      âŒ Output {output} out of range [0, {self.meta.dvoc})")
                        
                except Exception as e:
                    print(f"      âŒ Error with token {token}: {e}")
                finally:
                    try:
                        task._kv_cache.drop()
                    except AttributeError:
                        # å¦‚æœdropæ–¹æ³•æœ‰é—®é¢˜ï¼Œè·³è¿‡æ¸…ç†
                        print("    âš  KV cache cleanup skipped (method issue)")
                    except Exception as e:
                        print(f"    âš  KV cache cleanup failed: {e}")

        print(f"\n{'='*60}")
        print("ğŸ”¬ DIAGNOSIS COMPLETE")
        print(f"{'='*60}")
    
    def generate_simple(self, input_content, max_steps, topp_=0.8, topk_=50, temperature_=0.7):
        """ä¸ä½¿ç”¨chat templateçš„ç®€å•ç”Ÿæˆ"""
        print(f"\nSimple generation: '{input_content}'", end="", flush=True)
        tokens = self.tokenizer.encode(input_content)
    
        print(f"\nInput tokens: {tokens}")
        
        infer_task = Qwen3InferTask(
            tokens=tokens,
            position=0,
            temperature=temperature_,
            topk=topk_,
            topp=topp_,
            end_tokens=self.eos_token_id,
            max_tokens=int(self.meta.dctx),
            task_id=0
        )
    
        infer_task.bind_kvcache(Qwen3KVCache(self))
    
        output_content = ""
        for step_i in range(max_steps):
            output_tokens = self.batch_infer_one_round([infer_task])
            output_token = output_tokens[0]
            
            print(f" -> {output_token}", end="")
            
            if output_token >= self.meta.dvoc or output_token < 0:
                print(f" (INVALID)")
                break
            
            try:
                output_str = self.tokenizer.decode([output_token], skip_special_tokens=False)
            except Exception:
                output_str = f"[UNK_{output_token}]"
            
            output_content += output_str
            print(f"('{output_str}')", end="", flush=True)
            
            if output_token in self.eos_token_id:
                break
                
            infer_task.next(output_token)
    
        print(f"\nFinal output: '{output_content}'")
        try:
            infer_task._kv_cache.drop()
        except AttributeError:
            # å¦‚æœdropæ–¹æ³•æœ‰é—®é¢˜ï¼Œè·³è¿‡æ¸…ç†
            print("    âš  KV cache cleanup skipped (method issue)")
        except Exception as e:
            print(f"    âš  KV cache cleanup failed: {e}")
        return output_content
def test():
    if len(sys.argv) < 2:
        print("Usage: python qwen3_fixed.py <path/to/model_dir> [device] [n_device]")
        sys.exit(1)
        
    model_path = sys.argv[1]
    device_type = DeviceType.DEVICE_TYPE_CPU
    
    if len(sys.argv) > 2:
        if sys.argv[2] == "--cpu":
            device_type = DeviceType.DEVICE_TYPE_CPU
        elif sys.argv[2] == "--nvidia":
            device_type = DeviceType.DEVICE_TYPE_NVIDIA

    ndev = int(sys.argv[3]) if len(sys.argv) > 3 else 1
    
    print(f"âœ“ Using Qwen3 model from: {model_path}")
    print(f"âœ“ Device: {device_type}, Devices: {ndev}")
    
    model = QwenForCausalLM(model_path, device_type, ndev)
    
    # è¯Šæ–­C++è®¡ç®—é—®é¢˜
    model.diagnose_cpp_computation()
    
    # ç„¶åæµ‹è¯•ç”Ÿæˆ
    model.generate("å±±ä¸œæœ€é«˜çš„å±±æ˜¯ï¼Ÿ", 5, topp_=0.8, topk_=50, temperature_=0.7)
    model.destroy_model_instance()


if __name__ == "__main__":
    test()